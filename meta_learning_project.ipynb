{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcvonrohr/DeepLearning/blob/main/meta_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54242527",
      "metadata": {
        "id": "54242527",
        "outputId": "bb1ec5bc-b721-4f7a-b5b7-7a6112d010d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "...Google Drive connected.\n",
            "Local data directory created at: /content/data/inaturalist_unpacked\n",
            "\n",
            "--- Processing 2021_train_mini ---\n",
            "Copying '2021_train_mini.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 1056.15 seconds.\n",
            "Unpacking '2021_train_mini.tar.gz' locally...\n",
            "...Unpacking complete. Took 441.59 seconds.\n",
            "Deleting local tarball '/content/data/2021_train_mini.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Processing 2021_valid ---\n",
            "Copying '2021_valid.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 213.07 seconds.\n",
            "Unpacking '2021_valid.tar.gz' locally...\n",
            "...Unpacking complete. Took 77.47 seconds.\n",
            "Deleting local tarball '/content/data/2021_valid.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Final Data Setup Verification ---\n",
            "Dataset is ready for training at: /content/data/inaturalist_unpacked\n",
            "total 2.5M\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 train_mini\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 val\n",
            "\n",
            "Local VM Disk Space Usage:\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   98G  139G  42% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              41G     0   41G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "/dev/sda1       242G  150G   93G  62% /opt/bin/.nvidia\n",
            "tmpfs            42G  1.8M   42G   1% /var/colab\n",
            "tmpfs            42G     0   42G   0% /proc/acpi\n",
            "tmpfs            42G     0   42G   0% /proc/scsi\n",
            "tmpfs            42G     0   42G   0% /sys/firmware\n",
            "drive           236G  105G  132G  45% /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "#################################################################\n",
        "#  STEP 2.1: PREPARE LOCAL VM\n",
        "#################################################################\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "print(\"Connecting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"...Google Drive connected.\")\n",
        "\n",
        "# --- 2. Define Key Paths ---\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "DATASETS_ROOT_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
        "INAT_ROOT_DIR = os.path.join(DATASETS_ROOT_DIR, 'inaturalist')\n",
        "\n",
        "# Source: The COMPRESSED archives\n",
        "ARCHIVES_DIR_ON_DRIVE = os.path.join(INAT_ROOT_DIR, 'archives')\n",
        "\n",
        "# Target: The LOCAL VM fast disk\n",
        "LOCAL_DATA_ROOT = '/content/data'\n",
        "# This is the final path your PyTorch code will use:\n",
        "FINAL_DATA_PATH = os.path.join(LOCAL_DATA_ROOT, 'inaturalist_unpacked')\n",
        "\n",
        "# Define source/destination paths\n",
        "TAR_FILES = {\n",
        "    \"2021_train_mini\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_train_mini.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_train_mini.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_train_mini')\n",
        "    },\n",
        "    \"2021_valid\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_valid.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_valid.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_valid')\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 3. Create Local Directories on VM ---\n",
        "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(FINAL_DATA_PATH, exist_ok=True)\n",
        "print(f\"Local data directory created at: {FINAL_DATA_PATH}\")\n",
        "\n",
        "# --- 4. Copy, Unpack, and Clean up for each file ---\n",
        "for name, paths in TAR_FILES.items():\n",
        "    print(f\"\\n--- Processing {name} ---\")\n",
        "\n",
        "    if os.path.exists(paths[\"check_unpacked\"]):\n",
        "        print(f\"'{name}' is already unpacked in local VM. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 4a. Copy .tar.gz from Drive to local VM\n",
        "    print(f\"Copying '{name}.tar.gz' from Drive to local VM...\")\n",
        "    start_time = time.time()\n",
        "    !cp \"{paths['src']}\" \"{paths['dest_tar']}\"\n",
        "    print(f\"...Copy complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4b. Unpack the file on the local VM\n",
        "    print(f\"Unpacking '{name}.tar.gz' locally...\")\n",
        "    start_time = time.time()\n",
        "    !tar -xzf \"{paths['dest_tar']}\" -C \"{FINAL_DATA_PATH}\"\n",
        "    print(f\"...Unpacking complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4c. Delete the local .tar.gz file to save VM space\n",
        "    print(f\"Deleting local tarball '{paths['dest_tar']}'...\")\n",
        "    !rm \"{paths['dest_tar']}\"\n",
        "    print(\"...Local tarball deleted.\")\n",
        "\n",
        "# --- 5. Verify and Set Path for Training ---\n",
        "print(\"\\n--- Final Data Setup Verification ---\")\n",
        "print(f\"Dataset is ready for training at: {FINAL_DATA_PATH}\")\n",
        "!ls -lh \"{FINAL_DATA_PATH}\"\n",
        "print(\"\\nLocal VM Disk Space Usage:\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.2: SCIENTIFIC DATA PARTITIONING\n",
        "#################################################################\n",
        "print(\"\\n--- STEP 2.2: Loading/Creating Scientific Class Partition ---\")\n",
        "\n",
        "# --- 6. Define Paths for Partition File ---\n",
        "# We create a 'project_meta' folder on GDrive to store helper files\n",
        "META_DIR_ON_DRIVE = os.path.join(PROJECT_DIR, 'project_meta')\n",
        "os.makedirs(META_DIR_ON_DRIVE, exist_ok=True)\n",
        "\n",
        "PARTITION_FILE_PATH = os.path.join(META_DIR_ON_DRIVE, 'inat_class_split.json')\n",
        "print(f\"Looking for partition file at: {PARTITION_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_ARRY2GUvXt",
        "outputId": "b4ba3eb7-3994-4dd1-81d0-bdb3e668c922"
      },
      "id": "1_ARRY2GUvXt",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.2: Loading/Creating Scientific Class Partition ---\n",
            "Looking for partition file at: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Logic to Find Classes and Create Partition ---\n",
        "\n",
        "# 7a. Identify the Dataset Root\n",
        "# The unpacking might have created a subfolder (e.g., '2021_train_mini' or 'train_mini')\n",
        "# or files might be directly in FINAL_DATA_PATH. We check common patterns.\n",
        "possible_roots = [\n",
        "    os.path.join(FINAL_DATA_PATH, '2021_train_mini'),\n",
        "    os.path.join(FINAL_DATA_PATH, 'train_mini'),\n",
        "    FINAL_DATA_PATH\n",
        "]\n",
        "\n",
        "DATASET_ROOT = None\n",
        "for path in possible_roots:\n",
        "    if os.path.exists(path):\n",
        "        # Check if this path actually contains subdirectories\n",
        "        if len([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]) > 0:\n",
        "            DATASET_ROOT = path\n",
        "            break\n",
        "\n",
        "print(f\"Dataset root identified as: {DATASET_ROOT}\")\n",
        "\n",
        "# 7b. Load or Create the Partition\n",
        "partition_data = {}\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "if os.path.exists(PARTITION_FILE_PATH):\n",
        "    print(\"Found existing partition file. Loading...\")\n",
        "    with open(PARTITION_FILE_PATH, 'r') as f:\n",
        "        partition_data = json.load(f)\n",
        "else:\n",
        "    print(\"No partition file found. Scanning directories to create new partition...\")\n",
        "    print(\"This ensures independence from missing metadata files.\")\n",
        "\n",
        "    # --- Scan for Class Folders ---\n",
        "    class_folders_rel = []\n",
        "\n",
        "    # Walk through the directory tree\n",
        "    # A \"class\" is any folder that contains image files (.jpg, .jpeg, .png)\n",
        "    print(\"Scanning folders (this may take 1-2 minutes)...\")\n",
        "    for root, dirs, files in os.walk(DATASET_ROOT):\n",
        "        # Check for images in this specific folder\n",
        "        images = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if len(images) > 0:\n",
        "            # Get path relative to the dataset root (e.g., \"Aves/Turdus_migratorius\")\n",
        "            rel_path = os.path.relpath(root, DATASET_ROOT)\n",
        "            class_folders_rel.append(rel_path)\n",
        "\n",
        "    # --- CRITICAL: Sort for Reproducibility ---\n",
        "    # Sorting ensures that Index 0 is ALWAYS the same class on every machine/run\n",
        "    class_folders_rel.sort()\n",
        "\n",
        "    num_classes = len(class_folders_rel)\n",
        "    print(f\"Found {num_classes} classes containing images.\")\n",
        "\n",
        "    if num_classes < 9900:\n",
        "        print(\"WARNING: Found significantly fewer than 10,000 classes. Check extraction.\")\n",
        "\n",
        "    # --- Assign IDs and Shuffle ---\n",
        "    all_class_ids = list(range(num_classes))\n",
        "\n",
        "    print(f\"Shuffling {num_classes} class IDs with random seed {RANDOM_SEED}...\")\n",
        "    random.seed(RANDOM_SEED)\n",
        "    random.shuffle(all_class_ids)\n",
        "\n",
        "    # --- Split into Sets ---\n",
        "    # 6000 Base (Train/Meta-Train), 2000 Val (Hyperparams), 2000 Novel (Test)\n",
        "    c_base_ids = all_class_ids[:6000]\n",
        "    c_val_ids = all_class_ids[6000:8000]\n",
        "    c_novel_ids = all_class_ids[8000:]\n",
        "\n",
        "    # --- Construct Data Structure ---\n",
        "    # We save both the sets AND the mapping from ID -> Folder Path\n",
        "    partition_data = {\n",
        "        \"sets\": {\n",
        "            'c_base': sorted(c_base_ids),\n",
        "            'c_val': sorted(c_val_ids),\n",
        "            'c_novel': sorted(c_novel_ids)\n",
        "        },\n",
        "        \"id_to_path\": {\n",
        "            str(i): folder_path for i, folder_path in enumerate(class_folders_rel)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    print(f\"Saving new partition and mapping to: {PARTITION_FILE_PATH}\")\n",
        "    with open(PARTITION_FILE_PATH, 'w') as f:\n",
        "        json.dump(partition_data, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987lbsNFU4Qf",
        "outputId": "caaa5891-8a54-4609-db90-ff2487150b8b"
      },
      "id": "987lbsNFU4Qf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset root identified as: /content/data/inaturalist_unpacked/train_mini\n",
            "Found existing partition file. Loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Verification ---\n",
        "print(\"\\n--- Partitioning Complete ---\")\n",
        "sets = partition_data['sets']\n",
        "print(f\"Total C_base classes:  {len(sets['c_base'])}\")\n",
        "print(f\"Total C_val classes:   {len(sets['c_val'])}\")\n",
        "print(f\"Total C_novel classes: {len(sets['c_novel'])}\")\n",
        "\n",
        "# Check for overlaps (should be 0)\n",
        "base_set = set(sets['c_base'])\n",
        "val_set = set(sets['c_val'])\n",
        "novel_set = set(sets['c_novel'])\n",
        "\n",
        "overlap_bv = base_set & val_set\n",
        "overlap_bn = base_set & novel_set\n",
        "overlap_vn = val_set & novel_set\n",
        "\n",
        "print(f\"Overlap (Base-Val):    {len(overlap_bv)}\")\n",
        "print(f\"Overlap (Base-Novel):  {len(overlap_bn)}\")\n",
        "print(f\"Overlap (Val-Novel):   {len(overlap_vn)}\")\n",
        "\n",
        "if len(overlap_bv) + len(overlap_bn) + len(overlap_vn) == 0:\n",
        "    print(\"\\nSUCCESS: Classes are cleanly partitioned.\")\n",
        "else:\n",
        "    print(\"\\nCRITICAL ERROR: Overlaps detected in class sets!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8xIL6YPU6OM",
        "outputId": "844ca9fb-d659-41ad-d745-3f5f5ed2f681"
      },
      "id": "w8xIL6YPU6OM",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Partitioning Complete ---\n",
            "Total C_base classes:  6000\n",
            "Total C_val classes:   2000\n",
            "Total C_novel classes: 2000\n",
            "Overlap (Base-Val):    0\n",
            "Overlap (Base-Novel):  0\n",
            "Overlap (Val-Novel):   0\n",
            "\n",
            "SUCCESS: Classes are cleanly partitioned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.3: MODULAR DATA LOADERS (NO LEARN2LEARN DEPENDENCY)\n",
        "#################################################################\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "print(\"\\n--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\")\n",
        "\n",
        "# --- SAFETY CHECK ---\n",
        "# Ensure variables from Step 2.2 exist\n",
        "required_vars = ['DATASET_ROOT', 'PARTITION_FILE_PATH']\n",
        "if not all(v in globals() for v in required_vars):\n",
        "    raise NameError(f\"Missing variables from Step 2.2. Please run the previous cell.\")\n",
        "\n",
        "print(f\"Using Dataset Root: {DATASET_ROOT}\")\n",
        "print(f\"Using Partition File: {PARTITION_FILE_PATH}\")\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZE_STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ7TNDctB_kt",
        "outputId": "2bb5fe45-a449-43f9-a76d-0949e71164b0"
      },
      "id": "ZZ7TNDctB_kt",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\n",
            "Using Dataset Root: /content/data/inaturalist_unpacked/train_mini\n",
            "Using Partition File: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  CORE COMPONENT: The Custom Dataset Class\n",
        "# ==============================================================================\n",
        "class MetaINatDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset that enforces the scientific partition.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, partition_file, split='c_base', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        with open(partition_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if split not in data['sets']:\n",
        "            raise ValueError(f\"Invalid split '{split}'. Available: {list(data['sets'].keys())}\")\n",
        "\n",
        "        self.allowed_ids = data['sets'][split]\n",
        "        self.id_to_path = data['id_to_path']\n",
        "\n",
        "        # Map original ID -> 0..N-1\n",
        "        self.label_map = {orig: new for new, orig in enumerate(self.allowed_ids)}\n",
        "\n",
        "        self.samples = []\n",
        "        for original_id in self.allowed_ids:\n",
        "            rel_path = self.id_to_path[str(original_id)]\n",
        "            abs_path = os.path.join(self.root_dir, rel_path)\n",
        "            if os.path.exists(abs_path):\n",
        "                for img in os.listdir(abs_path):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        self.samples.append({\n",
        "                            'path': os.path.join(abs_path, img),\n",
        "                            'label': self.label_map[original_id]\n",
        "                        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = Image.open(sample['path']).convert('RGB')\n",
        "        label = sample['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ykdP8nDFcU6i"
      },
      "id": "ykdP8nDFcU6i",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  HELPER: Episodic Batch Generator (Replaces learn2learn)\n",
        "# ==============================================================================\n",
        "class EpisodicTaskGenerator:\n",
        "    \"\"\"\n",
        "    Native PyTorch implementation of an N-Way K-Shot task sampler.\n",
        "    Replaces learn2learn functionality without installation issues.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, ways, shots, query_shots):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Group all image indices by their label for fast sampling\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl not in self.indices_by_label:\n",
        "                self.indices_by_label[lbl] = []\n",
        "            self.indices_by_label[lbl].append(idx)\n",
        "\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # 1. Sample N random classes (Ways)\n",
        "        selected_classes = random.sample(self.classes, self.ways)\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # 2. Sample K + Q images from each class\n",
        "        for local_label, global_label_idx in enumerate(selected_classes):\n",
        "            indices = self.indices_by_label[global_label_idx]\n",
        "\n",
        "            # Ensure we have enough images, otherwise sample with replacement\n",
        "            needed = self.shots + self.query_shots\n",
        "            if len(indices) >= needed:\n",
        "                selected_indices = random.sample(indices, needed)\n",
        "            else:\n",
        "                selected_indices = random.choices(indices, k=needed)\n",
        "\n",
        "            # 3. Load images and re-label them to 0..N-1 for the episode\n",
        "            for idx in selected_indices:\n",
        "                img, _ = self.dataset[idx] # dataset returns (img, global_label)\n",
        "                batch_images.append(img)\n",
        "                # Important: The label for the loss function must be 0..Ways-1\n",
        "                batch_labels.append(local_label)\n",
        "\n",
        "        # Stack into a single tensor: [Ways * (Shots+Query), C, H, W]\n",
        "        data = torch.stack(batch_images)\n",
        "        labels = torch.tensor(batch_labels)\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def sample(self):\n",
        "        # Compatibility method to look like learn2learn\n",
        "        return self.__next__()"
      ],
      "metadata": {
        "id": "asZAUP18cfqQ"
      },
      "id": "asZAUP18cfqQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER A: Standard Pre-Training Loader\n",
        "# ==============================================================================\n",
        "def get_standard_loader(split='c_base', batch_size=64, shuffle=True):\n",
        "    print(f\"\\n[Loader A] Initializing Standard Loader for split '{split}'...\")\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=train_transforms)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\" -> {len(dataset)} total images.\")\n",
        "    print(f\" -> {len(dataset.allowed_ids)} classes.\")\n",
        "    return loader, len(dataset.allowed_ids)"
      ],
      "metadata": {
        "id": "oVtAtf9zcm9e"
      },
      "id": "oVtAtf9zcm9e",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER B: Episodic Task Loader (MAML) - NATIVE IMPLEMENTATION\n",
        "# ==============================================================================\n",
        "def get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1, img_size=84):\n",
        "    print(f\"\\n[Loader B] Initializing Episodic Generator for split '{split}'...\")\n",
        "\n",
        "    maml_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=maml_transforms)\n",
        "\n",
        "    # Use our native generator instead of learn2learn\n",
        "    task_generator = EpisodicTaskGenerator(\n",
        "        dataset,\n",
        "        ways=ways,\n",
        "        shots=shots,\n",
        "        query_shots=query_shots\n",
        "    )\n",
        "\n",
        "    print(f\" -> Configured {ways}-Way {shots}-Shot Tasks (Native PyTorch).\")\n",
        "    return task_generator"
      ],
      "metadata": {
        "id": "madMlUDOc5CK"
      },
      "id": "madMlUDOc5CK",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER C: Fixed Few-Shot Loader for FT/LoRA\n",
        "# ==============================================================================\n",
        "def get_fixed_few_shot_task(split='c_novel', ways=5, shots=1, query_shots=15, seed=None):\n",
        "    print(f\"\\n[Loader C] Creating Fixed Few-Shot Task from '{split}'...\")\n",
        "\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    eval_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=eval_transforms)\n",
        "\n",
        "    available_labels = list(set(s['label'] for s in dataset.samples))\n",
        "    selected_classes = random.sample(available_labels, ways)\n",
        "\n",
        "    class_indices = {c: [] for c in selected_classes}\n",
        "    for idx, sample in enumerate(dataset.samples):\n",
        "        if sample['label'] in selected_classes:\n",
        "            class_indices[sample['label']].append(idx)\n",
        "\n",
        "    support_indices = []\n",
        "    query_indices = []\n",
        "\n",
        "    for c in selected_classes:\n",
        "        idxs = class_indices[c]\n",
        "        random.shuffle(idxs)\n",
        "        support_indices.extend(idxs[:shots])\n",
        "        query_indices.extend(idxs[shots : shots+query_shots])\n",
        "\n",
        "    support_loader = DataLoader(Subset(dataset, support_indices), batch_size=16, shuffle=True)\n",
        "    query_loader = DataLoader(Subset(dataset, query_indices), batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\" -> Support Set: {len(support_indices)} images, Query Set: {len(query_indices)} images\")\n",
        "    return support_loader, query_loader"
      ],
      "metadata": {
        "id": "G1m0luxjc5gB"
      },
      "id": "G1m0luxjc5gB",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  VERIFICATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Testing Loaders ---\")\n",
        "\n",
        "# Test A\n",
        "try:\n",
        "    l_std, n_cls = get_standard_loader(split='c_base', batch_size=4)\n",
        "    print(\"Loader A (Standard) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader A Failed: {e}\")\n",
        "\n",
        "# Test B (Now using Native Generator)\n",
        "try:\n",
        "    task_gen = get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1)\n",
        "    batch_data, batch_labels = task_gen.sample()\n",
        "    # Expected shape: [Way*(Shot+Query), 3, 84, 84] -> [5*(1+1), 3, 84, 84] = [10, 3, 84, 84]\n",
        "    print(f\"Loader B (Episodic) check: OK. Batch shape: {batch_data.shape}\")\n",
        "    if batch_labels.max() >= 5:\n",
        "        print(\"WARNING: Labels not properly remapped to 0..N-1\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader B Failed: {e}\")\n",
        "\n",
        "# Test C\n",
        "try:\n",
        "    sup_dl, q_dl = get_fixed_few_shot_task(split='c_novel', ways=5, shots=5)\n",
        "    print(\"Loader C (Fixed) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader C Failed: {e}\")\n",
        "\n",
        "print(\"\\nStep 2.3 Complete (Dependencies Fixed).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zceGZ_SdEeE",
        "outputId": "2821831c-a697-416a-873e-10be4e6ed81e"
      },
      "id": "3zceGZ_SdEeE",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Loaders ---\n",
            "\n",
            "[Loader A] Initializing Standard Loader for split 'c_base'...\n",
            " -> 300000 total images.\n",
            " -> 6000 classes.\n",
            "Loader A (Standard) check: OK.\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 1-Shot Tasks (Native PyTorch).\n",
            "Loader B (Episodic) check: OK. Batch shape: torch.Size([10, 3, 84, 84])\n",
            "\n",
            "[Loader C] Creating Fixed Few-Shot Task from 'c_novel'...\n",
            " -> Support Set: 25 images, Query Set: 75 images\n",
            "Loader C (Fixed) check: OK.\n",
            "\n",
            "Step 2.3 Complete (Dependencies Fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 4: INTELLIGENT PRE-TRAINING (MAX PERF & MEMORY SAFE)\n",
        "#################################################################\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "import gc  # <--- WICHTIG für Garbage Collection\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\")\n",
        "\n",
        "# --- 0. DRIVE & PATH SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. SEED SETUP ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# --- 2. HARDWARE DETECTION (TUNED FOR A100) ---\n",
        "def get_optimal_config():\n",
        "    cpu_count = os.cpu_count()\n",
        "    optimal_workers = min(cpu_count, 8)\n",
        "    device_name = \"CPU\"\n",
        "    batch_size = 16\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_name = gpu_name\n",
        "        # --- TUNING ---\n",
        "        if \"A100\" in gpu_name:\n",
        "            batch_size = 512  # <--- Aggressiver für A100 (40GB VRAM erlaubt das locker)\n",
        "        elif \"T4\" in gpu_name:\n",
        "            batch_size = 128\n",
        "        else:\n",
        "            batch_size = 64\n",
        "    else:\n",
        "        print(\"WARNING: No GPU detected!\")\n",
        "\n",
        "    return device_name, batch_size, optimal_workers\n",
        "\n",
        "detected_device, auto_bs, auto_workers = get_optimal_config()\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "\n",
        "    # --- CONTROL CENTER ---\n",
        "    'DRY_RUN': False,            # <--- REAL TRAINING\n",
        "    'NUM_EPOCHS': 20,\n",
        "    # ----------------------\n",
        "\n",
        "    'BATCH_SIZE': auto_bs,\n",
        "    'NUM_WORKERS': auto_workers,\n",
        "    'DEVICE_NAME': detected_device,\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'PATIENCE': 5,\n",
        "    'SUBSETS': [0.25, 0.50, 1.0],\n",
        "\n",
        "    'CHECKPOINT_DIR_LOC': '/content/checkpoints',\n",
        "    'CHECKPOINT_DIR_DRIVE': MODELS_DIR\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG['CHECKPOINT_DIR_LOC'], exist_ok=True)\n",
        "\n",
        "print(f\"\\nSystem Configuration:\")\n",
        "print(f\" -> Hardware:    {CONFIG['DEVICE_NAME']}\")\n",
        "print(f\" -> Batch Size:  {CONFIG['BATCH_SIZE']} (Optimized)\")\n",
        "print(f\" -> Workers:     {CONFIG['NUM_WORKERS']}\")\n",
        "print(f\" -> Mode:        {'DRY RUN' if CONFIG['DRY_RUN'] else 'REAL TRAINING'}\")\n",
        "\n",
        "# --- 4. MEMORY CLEANUP HELPER (NEW) ---\n",
        "def cleanup_memory():\n",
        "    \"\"\"Forces Garbage Collection and clears GPU Cache.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # Optional: Print stats to verify\n",
        "    # print(f\"   [Mem] Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# --- 5. MODEL FACTORY ---\n",
        "def get_base_model(arch_name, num_classes, pretrained=True):\n",
        "    # Loading logic same as before\n",
        "    if arch_name == 'resnet34':\n",
        "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet18':\n",
        "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet50':\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    else:\n",
        "        raise ValueError(\"Arch not supported\")\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# --- 6. DATA LOADER HELPER ---\n",
        "def get_subset_loader(fraction):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    full_ds = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split='c_base', transform=train_transforms)\n",
        "\n",
        "    total_base_classes = len(full_ds.allowed_ids)\n",
        "    target_num = int(total_base_classes * fraction)\n",
        "    subset_ids = full_ds.allowed_ids[:target_num]\n",
        "\n",
        "    # Filter samples (Memory efficient filtering logic)\n",
        "    # We recreate the list to drop references to unused samples\n",
        "    new_samples = [s for s in full_ds.samples if s['label'] < target_num]\n",
        "    full_ds.samples = new_samples\n",
        "    full_ds.allowed_ids = subset_ids\n",
        "    full_ds.label_map = {orig: new for new, orig in enumerate(subset_ids)}\n",
        "\n",
        "    print(f\"\\n[Data] Subset {fraction*100}%: {len(new_samples)} images, {target_num} classes.\")\n",
        "\n",
        "    num_val = int(0.1 * len(full_ds))\n",
        "    train_ds, val_ds = random_split(full_ds, [len(full_ds)-num_val, num_val],\n",
        "                                    generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True,\n",
        "                              num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False,\n",
        "                            num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, target_num\n",
        "\n",
        "# --- 7. ROBUST CHECKPOINTING ---\n",
        "def safe_copy_to_drive(local_path, filename, max_retries=5):\n",
        "    drive_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], filename)\n",
        "    if not os.path.exists(CONFIG['CHECKPOINT_DIR_DRIVE']):\n",
        "        try: os.makedirs(CONFIG['CHECKPOINT_DIR_DRIVE'], exist_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            shutil.copy(local_path, drive_path)\n",
        "            if os.path.exists(drive_path) and os.path.getsize(drive_path) > 0:\n",
        "                print(f\"   -> Drive Copy: SUCCESS\")\n",
        "                return\n",
        "        except Exception as e:\n",
        "            wait_time = 3 * attempt\n",
        "            print(f\"   [Retry {attempt}] Copy failed ({e}). Waiting {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"   [CRITICAL ERROR] Failed to copy {filename} to Drive.\")\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(model.state_dict(), local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "# --- 8. TRAINING ENGINE ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, target_epochs, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    run_tag = \"_dryrun\" if CONFIG['DRY_RUN'] else \"\"\n",
        "    ckpt_filename = f\"{model_name}{run_tag}_checkpoint.pth\"\n",
        "    best_filename = f\"{model_name}{run_tag}_best.pth\"\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_acc = -1.0\n",
        "\n",
        "    # Resume Logic\n",
        "    drive_ckpt_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], ckpt_filename)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"\\n[RESUME] Found: {ckpt_filename}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(drive_ckpt_path, map_location=device)\n",
        "            saved_epoch = checkpoint['epoch']\n",
        "\n",
        "            if CONFIG['DRY_RUN']:\n",
        "                print(f\"   -> (Dry Run) Resetting loop despite found epoch {saved_epoch+1}.\")\n",
        "                start_epoch = 0\n",
        "                best_acc = checkpoint.get('best_acc', -1.0)\n",
        "            else:\n",
        "                if saved_epoch >= (target_epochs - 1):\n",
        "                    print(f\"   -> Fully trained ({saved_epoch+1} epochs). Skipping.\")\n",
        "                    return model\n",
        "                start_epoch = saved_epoch + 1\n",
        "                best_acc = checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            if 'scaler_state_dict' in checkpoint:\n",
        "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "            print(f\"   -> Resuming with Best Acc: {best_acc:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   [ERROR] Checkpoint corrupted ({e}). Fresh start.\")\n",
        "    else:\n",
        "        print(f\"\\n[START] Fresh start for {model_name}.\")\n",
        "\n",
        "    effective_epochs = 2 if CONFIG['DRY_RUN'] else target_epochs\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(start_epoch, effective_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{effective_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        limit_batches = 5 if CONFIG['DRY_RUN'] else None\n",
        "\n",
        "        pbar = tqdm(train_loader, leave=False, desc=\"Training\")\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if limit_batches and i >= limit_batches: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        iter_size = (limit_batches * CONFIG['BATCH_SIZE']) if limit_batches else len(train_loader.dataset)\n",
        "        if iter_size == 0: iter_size = 1\n",
        "        epoch_acc = running_corrects.double() / iter_size\n",
        "        epoch_loss = running_loss / iter_size\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        val_limit = 5 if CONFIG['DRY_RUN'] else None\n",
        "        val_count = 0\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            if val_limit and i >= val_limit: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "            val_count += inputs.size(0)\n",
        "        val_acc = val_corrects.double() / val_count if val_count > 0 else 0.0\n",
        "        print(f\"   Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save Checkpoint\n",
        "        full_state = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'best_acc': best_acc\n",
        "        }\n",
        "        save_checkpoint(full_state, ckpt_filename)\n",
        "\n",
        "        # Save Best Model logic\n",
        "        save_condition = False\n",
        "        if val_acc > best_acc: save_condition = True\n",
        "        elif CONFIG['DRY_RUN'] and val_acc >= best_acc: save_condition = True\n",
        "        elif best_acc == -1.0: save_condition = True\n",
        "\n",
        "        if save_condition:\n",
        "            best_acc = val_acc\n",
        "            save_best_model(model, best_filename)\n",
        "            print(f\"   [New Best] Saved {best_filename}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if not CONFIG['DRY_RUN'] and patience_counter >= CONFIG['PATIENCE']:\n",
        "            print(f\"   [Early Stopping] Reached patience limit.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training Finished. Final Best Acc: {best_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 9. EXECUTION LOOP (WITH CLEANUP) ---\n",
        "for fraction in CONFIG['SUBSETS']:\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    print(f\"\\n{'='*40}\\nRUN: {subset_name}\\n{'='*40}\")\n",
        "\n",
        "    train_dl, val_dl, num_cls = get_subset_loader(fraction)\n",
        "    model = get_base_model(CONFIG['ARCH'], num_classes=num_cls)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "    # Train\n",
        "    train_model(model, train_dl, val_dl, criterion, optimizer, lr_scheduler, CONFIG['NUM_EPOCHS'], subset_name)\n",
        "\n",
        "    # --- MEMORY CLEANUP ---\n",
        "    print(f\"   [Cleanup] Clearing GPU memory after {subset_name}...\")\n",
        "    del model\n",
        "    del optimizer\n",
        "    del criterion\n",
        "    del train_dl\n",
        "    del val_dl\n",
        "    cleanup_memory() # Call helper to force GC and Empty Cache\n",
        "    print(f\"   [Cleanup] Done. Ready for next model.\\n\")\n",
        "\n",
        "print(\"\\nPHASE 4 COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbiMjGzgloUw",
        "outputId": "ad288dc8-2c66-424f-e37e-9fbfe36ff51d"
      },
      "id": "BbiMjGzgloUw",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\n",
            "\n",
            "System Configuration:\n",
            " -> Hardware:    NVIDIA A100-SXM4-40GB\n",
            " -> Batch Size:  512 (Optimized)\n",
            " -> Workers:     8\n",
            " -> Mode:        REAL TRAINING\n",
            "\n",
            "========================================\n",
            "RUN: M_base_25\n",
            "========================================\n",
            "\n",
            "[Data] Subset 25.0%: 75000 images, 1500 classes.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 199MB/s]\n",
            "/tmp/ipython-input-3002377437.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESUME] Found: M_base_25_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_25...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_50\n",
            "========================================\n",
            "\n",
            "[Data] Subset 50.0%: 150000 images, 3000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_50_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_50...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_100\n",
            "========================================\n",
            "\n",
            "[Data] Subset 100.0%: 300000 images, 6000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_100_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_100...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "PHASE 4 COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Pfad anpassen falls nötig\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "MODELS_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning', 'models', 'base_models')\n",
        "\n",
        "print(f\"Lese Ergebnisse aus: {MODELS_DIR}\\n\")\n",
        "\n",
        "subsets = [25, 50, 100]\n",
        "\n",
        "for s in subsets:\n",
        "    # Wir suchen nach der _checkpoint Datei, da diese die Metadaten hat\n",
        "    filename = f\"M_base_{s}_checkpoint.pth\"\n",
        "    path = os.path.join(MODELS_DIR, filename)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            # Wir laden auf CPU, das geht schneller\n",
        "            checkpoint = torch.load(path, map_location='cpu')\n",
        "\n",
        "            acc = checkpoint.get('best_acc', -1)\n",
        "            epoch = checkpoint.get('epoch', -1)\n",
        "\n",
        "            print(f\"Modell {s}%:\")\n",
        "            print(f\"  -> Best Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "            print(f\"  -> Gestoppt nach Epoche: {epoch+1}\")\n",
        "            print(\"-\" * 30)\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Lesen von {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"WARNUNG: Checkpoint {filename} nicht gefunden. Nur _best.pth vorhanden?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtyyskInML8W",
        "outputId": "180d516f-9581-4bb7-d6b9-8b259c0dc73b"
      },
      "id": "XtyyskInML8W",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lese Ergebnisse aus: /content/drive/MyDrive/Deep Learning/models/base_models\n",
            "\n",
            "Modell 25%:\n",
            "  -> Best Accuracy: 0.4552 (45.52%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 50%:\n",
            "  -> Best Accuracy: 0.3140 (31.40%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 100%:\n",
            "  -> Best Accuracy: 0.3681 (36.81%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 5: MAML ULTIMATE (128px, 84GB RAM POWER, LOAD-ONCE)\n",
        "#################################################################\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import gc\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "print(\"\\n--- PHASE 5: Pipeline 1 - Meta-Learning (Ultimate Edition) ---\")\n",
        "\n",
        "# --- 0. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "BASE_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "MAML_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'maml_models')\n",
        "CHECKPOINT_DIR_LOC = '/content/checkpoints'\n",
        "\n",
        "os.makedirs(MAML_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR_LOC, exist_ok=True)\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MAML_CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "\n",
        "    # --- CONTROL CENTER ---\n",
        "    'DRY_RUN': False,            # <--- REAL RUN\n",
        "    'META_ITERATIONS': 2000,     # 2000 Iters bei Batch 16 sind sehr stark\n",
        "    'VAL_INTERVAL': 100,\n",
        "\n",
        "    # --- HIGH RES SETTINGS ---\n",
        "    'META_BATCH_SIZE': 16,       # Safe for A100 with 128px\n",
        "    'IMG_SIZE': 128,             # <--- UPGRADE: Mehr Details!\n",
        "    'PRELOAD_RAM': True,         # Nutzen wir deine 84GB RAM!\n",
        "    # -------------------------\n",
        "\n",
        "    # Your Tuned Params\n",
        "    'META_LR': 0.001,\n",
        "    'INNER_LR': 0.01,\n",
        "    'INNER_STEPS': 5,\n",
        "    'GRAD_CLIP': 1.0,\n",
        "    'WEIGHT_DECAY': 1e-4,\n",
        "\n",
        "    'WAYS': 5, 'SHOTS': 5, 'QUERY_SHOTS': 15,\n",
        "    'SUBSETS': [0.25, 0.50, 1.0]\n",
        "}\n",
        "\n",
        "if MAML_CONFIG['DRY_RUN']:\n",
        "    MAML_CONFIG['META_ITERATIONS'] = 5\n",
        "    MAML_CONFIG['VAL_INTERVAL'] = 1\n",
        "    MAML_CONFIG['META_BATCH_SIZE'] = 2\n",
        "\n",
        "print(f\"Config: {MAML_CONFIG['META_ITERATIONS']} Iters | Batch {MAML_CONFIG['META_BATCH_SIZE']} | Res {MAML_CONFIG['IMG_SIZE']}px\")\n",
        "\n",
        "\n",
        "# --- 2. DATASET (LOAD FULL SET INTO RAM) ---\n",
        "class CachedMetaINatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, partition_file, split='c_base', transform=None, preload=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(partition_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        self.allowed_ids = data['sets'][split]\n",
        "        self.id_to_path = data['id_to_path']\n",
        "        self.samples = []\n",
        "        self.label_map = {orig: new for new, orig in enumerate(self.allowed_ids)}\n",
        "\n",
        "        for original_id in self.allowed_ids:\n",
        "            rel_path = self.id_to_path[str(original_id)]\n",
        "            abs_path = os.path.join(self.root_dir, rel_path)\n",
        "            if os.path.exists(abs_path):\n",
        "                for img in os.listdir(abs_path):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        self.samples.append({\n",
        "                            'path': os.path.join(abs_path, img),\n",
        "                            'label': self.label_map[original_id]\n",
        "                        })\n",
        "\n",
        "        self.cache = {}\n",
        "        self.preload = preload\n",
        "        if self.preload:\n",
        "            print(f\"   [RAM] Loading {len(self.samples)} images ({MAML_CONFIG['IMG_SIZE']}px) for '{split}'...\")\n",
        "            # We store PIL images to keep RAM usage compact (uint8)\n",
        "            # Transform happens on-the-fly during training loop\n",
        "            for i, s in enumerate(tqdm(self.samples, desc=\"Caching\")):\n",
        "                img = Image.open(s['path']).convert('RGB')\n",
        "                img = img.resize((MAML_CONFIG['IMG_SIZE'], MAML_CONFIG['IMG_SIZE']))\n",
        "                self.cache[i] = img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.preload:\n",
        "            image = self.cache[idx]\n",
        "        else:\n",
        "            image = Image.open(self.samples[idx]['path']).convert('RGB')\n",
        "            image = image.resize((MAML_CONFIG['IMG_SIZE'], MAML_CONFIG['IMG_SIZE']))\n",
        "\n",
        "        label = self.samples[idx]['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# --- 3. GENERATOR WITH SUBSET FILTER ---\n",
        "class A100TaskGenerator:\n",
        "    def __init__(self, dataset, ways, shots, query_shots, fraction=1.0):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # SECURITY: Calculate allowed class range\n",
        "        total_classes = len(dataset.allowed_ids)\n",
        "        allowed_limit = int(total_classes * fraction)\n",
        "        print(f\"   [Gen] Filter: Using {allowed_limit} of {total_classes} classes ({fraction*100}%).\")\n",
        "\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            # ONLY index classes within the allowed range!\n",
        "            if lbl < allowed_limit:\n",
        "                if lbl not in self.indices_by_label: self.indices_by_label[lbl] = []\n",
        "                self.indices_by_label[lbl].append(idx)\n",
        "\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def sample_batch(self, batch_size=16):\n",
        "        all_data, all_labels = [], []\n",
        "        for _ in range(batch_size):\n",
        "            selected_classes = random.sample(self.classes, self.ways)\n",
        "            task_imgs, task_lbls = [], []\n",
        "\n",
        "            for local_label, global_label in enumerate(selected_classes):\n",
        "                indices = self.indices_by_label[global_label]\n",
        "                needed = self.shots + self.query_shots\n",
        "                selected = random.sample(indices, needed) if len(indices) >= needed else random.choices(indices, k=needed)\n",
        "\n",
        "                for idx in selected:\n",
        "                    img, _ = self.dataset[idx]\n",
        "                    task_imgs.append(img)\n",
        "                    task_lbls.append(local_label)\n",
        "            all_data.append(torch.stack(task_imgs))\n",
        "            all_labels.append(torch.tensor(task_lbls))\n",
        "        return torch.stack(all_data), torch.stack(all_labels)\n",
        "\n",
        "\n",
        "# --- 4. GLOBAL INIT (RUN ONCE) ---\n",
        "print(\"\\n>>> LOADING DATA TO RAM (One-Time Cost) <<<\")\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# NOTE: Resize is already done in Caching. Only ToTensor+Norm here.\n",
        "global_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(NORM_MEAN, NORM_STD)\n",
        "])\n",
        "\n",
        "# Global Objects\n",
        "GLOBAL_TRAIN_DS = CachedMetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split='c_base', transform=global_transform, preload=True)\n",
        "GLOBAL_VAL_DS = CachedMetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split='c_val', transform=global_transform, preload=True)\n",
        "print(\">>> RAM LOADING COMPLETE <<<\\n\")\n",
        "\n",
        "\n",
        "# --- 5. HELPERS & ENGINE ---\n",
        "def safe_copy_to_drive(local_path, filename):\n",
        "    drive_path = os.path.join(MAML_MODELS_DIR, filename)\n",
        "    try: shutil.copy(local_path, drive_path)\n",
        "    except: pass\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model_state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(model_state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "    print(f\"   [New Best] Saved {filename}\")\n",
        "\n",
        "def load_base_model_for_maml(fraction, arch='resnet34'):\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    candidates = [f\"{subset_name}_best.pth\", f\"{subset_name}_checkpoint.pth\", f\"{subset_name}_dryrun_best.pth\"]\n",
        "    path = None\n",
        "    for c in candidates:\n",
        "        p = os.path.join(BASE_MODELS_DIR, c)\n",
        "        if os.path.exists(p): path = p; break\n",
        "    if path is None: raise FileNotFoundError(f\"No base model for {subset_name}\")\n",
        "\n",
        "    # Fix architecture creation\n",
        "    # Base models were trained with specific output heads (1500/3000/6000)\n",
        "    # We must match that to load weights, THEN replace head for MAML (or keep it?)\n",
        "    # MAML re-initializes head anyway for each task.\n",
        "    # BUT to load state_dict successfully, shapes must match.\n",
        "    full_classes = 6000\n",
        "    num_classes = int(full_classes * fraction)\n",
        "\n",
        "    model = models.resnet34(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def optimized_maml_step(meta_model, tasks_data, tasks_labels, meta_optimizer, criterion, device, meta_batch_size):\n",
        "    meta_loss_total = 0.0\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "    support_indices, query_indices = [], []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    for i in range(meta_batch_size):\n",
        "        supp_X = tasks_data[i][support_indices]\n",
        "        supp_y = tasks_labels[i][support_indices]\n",
        "        query_X = tasks_data[i][query_indices]\n",
        "        query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train()\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "        for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        q_preds = fast_model(query_X)\n",
        "        q_loss = criterion(q_preds, query_y)\n",
        "        meta_loss_total += q_loss.item()\n",
        "        q_loss.backward()\n",
        "\n",
        "        for mp, fp in zip(meta_model.parameters(), fast_model.parameters()):\n",
        "            if fp.grad is not None:\n",
        "                grad = fp.grad.detach() / meta_batch_size\n",
        "                if mp.grad is None: mp.grad = grad\n",
        "                else: mp.grad += grad\n",
        "        del fast_model, inner_opt\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(meta_model.parameters(), MAML_CONFIG['GRAD_CLIP'])\n",
        "    meta_optimizer.step()\n",
        "    return meta_loss_total / meta_batch_size\n",
        "\n",
        "def evaluate_optimized(meta_model, val_generator, criterion, device):\n",
        "    meta_model.eval()\n",
        "    num_val_batches = 1 if MAML_CONFIG['DRY_RUN'] else 5\n",
        "    total_acc = 0.0\n",
        "    meta_batch = MAML_CONFIG['META_BATCH_SIZE']\n",
        "\n",
        "    # Same indices logic\n",
        "    ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "    support_indices, query_indices = [], []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    for _ in range(num_val_batches):\n",
        "        tasks_data, tasks_labels = val_generator.sample_batch(meta_batch)\n",
        "        tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "        batch_acc = 0.0\n",
        "\n",
        "        for i in range(meta_batch):\n",
        "            supp_X = tasks_data[i][support_indices]\n",
        "            supp_y = tasks_labels[i][support_indices]\n",
        "            query_X = tasks_data[i][query_indices]\n",
        "            query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "            fast_model = copy.deepcopy(meta_model)\n",
        "            fast_model.train()\n",
        "            inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "            for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "                preds = fast_model(supp_X)\n",
        "                loss = criterion(preds, supp_y)\n",
        "                inner_opt.zero_grad()\n",
        "                loss.backward()\n",
        "                inner_opt.step()\n",
        "\n",
        "            fast_model.eval()\n",
        "            with torch.no_grad():\n",
        "                q_preds = fast_model(query_X)\n",
        "                _, predicted = torch.max(q_preds.data, 1)\n",
        "                batch_acc += (predicted == query_y).sum().item() / query_y.size(0)\n",
        "            del fast_model, inner_opt\n",
        "\n",
        "        total_acc += batch_acc\n",
        "    meta_model.train()\n",
        "    return total_acc / (num_val_batches * meta_batch)\n",
        "\n",
        "\n",
        "# --- 6. RUN LOOP ---\n",
        "def run_maml_training(fraction):\n",
        "    maml_name = f\"M_maml_{int(fraction*100)}\"\n",
        "    run_tag = \"_dryrun\" if MAML_CONFIG['DRY_RUN'] else \"\"\n",
        "    ckpt_file = f\"{maml_name}{run_tag}_checkpoint.pth\"\n",
        "    best_file = f\"{maml_name}{run_tag}_best.pth\"\n",
        "\n",
        "    print(f\"\\n{'='*40}\\nMETA-TRAINING: {maml_name}{run_tag}\\n{'='*40}\")\n",
        "\n",
        "    # Create Filtered Generators from Global Objects\n",
        "    train_gen = A100TaskGenerator(GLOBAL_TRAIN_DS, MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS'], fraction=fraction)\n",
        "    val_gen = A100TaskGenerator(GLOBAL_VAL_DS, MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS'], fraction=1.0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    meta_model = load_base_model_for_maml(fraction, MAML_CONFIG['ARCH'])\n",
        "    meta_model = meta_model.to(device)\n",
        "\n",
        "    meta_optimizer = optim.AdamW(meta_model.parameters(), lr=MAML_CONFIG['META_LR'], weight_decay=MAML_CONFIG['WEIGHT_DECAY'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_iter = 0\n",
        "    best_val_acc = -1.0\n",
        "\n",
        "    drive_ckpt_path = os.path.join(MAML_MODELS_DIR, ckpt_file)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"[RESUME] Found {ckpt_file}\")\n",
        "        try:\n",
        "            ckpt = torch.load(drive_ckpt_path, map_location=device)\n",
        "            if MAML_CONFIG['DRY_RUN']:\n",
        "                 start_iter = 0\n",
        "                 best_val_acc = ckpt.get('best_val_acc', -1.0)\n",
        "            else:\n",
        "                 start_iter = ckpt['iteration'] + 1\n",
        "                 best_val_acc = ckpt.get('best_val_acc', 0.0)\n",
        "                 meta_model.load_state_dict(ckpt['model_state_dict'])\n",
        "                 meta_optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "                 print(f\"   -> Resuming from iter {start_iter} (Best Val Acc: {best_val_acc:.4f})\")\n",
        "        except Exception: pass\n",
        "\n",
        "    total_iters = 5 if MAML_CONFIG['DRY_RUN'] else MAML_CONFIG['META_ITERATIONS']\n",
        "    pbar = tqdm(range(start_iter, total_iters), desc=f\"Training\")\n",
        "\n",
        "    for i in pbar:\n",
        "        # 1. Fetch\n",
        "        tasks_data, tasks_labels = train_gen.sample_batch(MAML_CONFIG['META_BATCH_SIZE'])\n",
        "        tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "\n",
        "        # 2. Step\n",
        "        loss = optimized_maml_step(meta_model, tasks_data, tasks_labels, meta_optimizer, criterion, device, MAML_CONFIG['META_BATCH_SIZE'])\n",
        "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
        "\n",
        "        # 3. Validation\n",
        "        if i % MAML_CONFIG['VAL_INTERVAL'] == 0 or i == total_iters - 1:\n",
        "            val_acc = evaluate_optimized(meta_model, val_gen, criterion, device)\n",
        "            tqdm.write(f\"   Iter {i}: Meta Loss {loss:.4f} | Val Acc {val_acc:.4f} (Best: {best_val_acc:.4f})\")\n",
        "\n",
        "            state = {'iteration': i, 'model_state_dict': meta_model.state_dict(), 'optimizer_state_dict': meta_optimizer.state_dict(), 'best_val_acc': best_val_acc}\n",
        "            save_checkpoint(state, ckpt_file)\n",
        "\n",
        "            save_condition = False\n",
        "            if val_acc > best_val_acc: save_condition = True\n",
        "            elif best_val_acc == -1.0: save_condition = True\n",
        "            elif MAML_CONFIG['DRY_RUN'] and val_acc >= best_val_acc: save_condition = True\n",
        "\n",
        "            if save_condition:\n",
        "                best_val_acc = val_acc\n",
        "                save_best_model(meta_model.state_dict(), best_file)\n",
        "\n",
        "    print(f\"Training Complete. Best Val Acc: {best_val_acc:.4f}\")\n",
        "    del meta_model, meta_optimizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 7. EXECUTION ---\n",
        "for fraction in MAML_CONFIG['SUBSETS']:\n",
        "    try:\n",
        "        run_maml_training(fraction)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {fraction}: {e}\")\n",
        "\n",
        "print(\"\\nPHASE 5 COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7e1b01f947c5465f9d6d7075fad8c4fa",
            "5ab8a89c0d454c76827134212a9a8a92",
            "3457a26430f4421e800a450ef281a0cb",
            "407e9cbc55584baaa38fa60c71ec73b6",
            "eeed7935da014b539b3ccc381d7573da",
            "26e83b8fd5494051aeda3038e8cc8cea",
            "c1e01440edad449bb304cf05a684c541",
            "f7ee388c999b42bbaa77844a38840414",
            "19ee9ef2e4d04e56a8fced0a5654f137",
            "fc16b6ec11374322b98a05a49d480d51",
            "078016825940482b807bdb217a1477a0",
            "bc5a5c455d09406c814632f4274a3da4",
            "54064146be704ceea0a819cc9639ad36",
            "37816b20beaa4a23a1f20cbc166440d4",
            "6b2a8c1fb9b3433097092d4b8940c943",
            "42422a3bbda4494c8de0e6d36b26acfe",
            "0abe45d90f4543d8b7224f5926f962f3",
            "d7da4f2f1b9648799d926fefd935fa68",
            "2552765998a245debb5404f06e8b35fa",
            "0792ad57e2bb464f918aac6c789a59df",
            "2a1c46a880864aa2a65afc9ab6dbd53a",
            "f1a0f4a3eb5b4387b9d9202eb53aa39c",
            "158b12ae8fc34e92ae5c21a8c1837594",
            "0723d19165c84f5198f4f98beb0a84a8",
            "a6fa8bb0281a42e68993f968020286bf",
            "23e4f2f39d3b43bbbab5f6816fc64243",
            "9b04a012e7a641b8bf8d2aa348558e8a",
            "29bfc92b4c6244eabba7f709363caee5",
            "ae52634b8a3c48c5bf1ade4bcec712a8",
            "fd2ddc790ce54837831f6da0dcada2bd",
            "4349e57318384b0b81bd082a91611bee",
            "9e557682deef45818fba08509d2501a6",
            "4e9dfc2a5b864c0ca9d9a1a65b00c5b7",
            "c4e9b82aef5d41e0be54fa36b44dd609",
            "b461d8776fb044039a062a2fbc9f5c5a",
            "ddc1a725915b45cf8412309b48b1dcf2",
            "481242dd252a44d3a0eba7263db05645",
            "5ae9aeb800e141518ab3332f7a1dfab9",
            "40dceae64d6f4fe1b9693937258931cb",
            "67dfc263182b4fbe9ab83429949b2b3b",
            "98bde641b6b24edc974b748555634d89",
            "1d2a762892ad40dab1052349ce0d94c6",
            "4ddfc54c8daf4b74be14cf38dad7311b",
            "cab7e6e419204920a3f8d647e880d7b3",
            "67e3a7bbbd0040eb893ac01b7929e60e"
          ]
        },
        "id": "PLNuhzHGHswM",
        "outputId": "41347996-375c-4db0-e352-ab8dcf362226"
      },
      "id": "PLNuhzHGHswM",
      "execution_count": 20,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- PHASE 5: Pipeline 1 - Meta-Learning (Ultimate Edition) ---\n",
            "Config: 2000 Iters | Batch 16 | Res 128px\n",
            "\n",
            ">>> LOADING DATA TO RAM (One-Time Cost) <<<\n",
            "   [RAM] Loading 300000 images (128px) for 'c_base'...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e1b01f947c5465f9d6d7075fad8c4fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Caching:   0%|          | 0/300000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [RAM] Loading 100000 images (128px) for 'c_val'...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc5a5c455d09406c814632f4274a3da4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Caching:   0%|          | 0/100000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> RAM LOADING COMPLETE <<<\n",
            "\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_25\n",
            "========================================\n",
            "   [Gen] Filter: Using 1500 of 6000 classes (25.0%).\n",
            "   [Gen] Filter: Using 2000 of 2000 classes (100.0%).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "158b12ae8fc34e92ae5c21a8c1837594",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Iter 0: Meta Loss 12.8519 | Val Acc 0.0777 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_25_best.pth\n",
            "   Iter 100: Meta Loss 1.1671 | Val Acc 0.2557 (Best: 0.0777)\n",
            "   [New Best] Saved M_maml_25_best.pth\n",
            "   Iter 200: Meta Loss 1.0010 | Val Acc 0.2485 (Best: 0.2557)\n",
            "   Iter 300: Meta Loss 0.9863 | Val Acc 0.2247 (Best: 0.2557)\n",
            "   Iter 400: Meta Loss 0.9975 | Val Acc 0.3033 (Best: 0.2557)\n",
            "   [New Best] Saved M_maml_25_best.pth\n",
            "   Iter 500: Meta Loss 0.8342 | Val Acc 0.2710 (Best: 0.3033)\n",
            "   Iter 600: Meta Loss 0.8050 | Val Acc 0.2770 (Best: 0.3033)\n",
            "   Iter 700: Meta Loss 0.9004 | Val Acc 0.2322 (Best: 0.3033)\n",
            "   Iter 800: Meta Loss 0.8492 | Val Acc 0.2647 (Best: 0.3033)\n",
            "   Iter 900: Meta Loss 0.7568 | Val Acc 0.2785 (Best: 0.3033)\n",
            "   Iter 1000: Meta Loss 0.8523 | Val Acc 0.2130 (Best: 0.3033)\n",
            "   Iter 1100: Meta Loss 0.7583 | Val Acc 0.2287 (Best: 0.3033)\n",
            "   Iter 1200: Meta Loss 0.8198 | Val Acc 0.2125 (Best: 0.3033)\n",
            "   Iter 1300: Meta Loss 0.8495 | Val Acc 0.2405 (Best: 0.3033)\n",
            "   Iter 1400: Meta Loss 0.6241 | Val Acc 0.2380 (Best: 0.3033)\n",
            "   Iter 1500: Meta Loss 0.5726 | Val Acc 0.2267 (Best: 0.3033)\n",
            "   Iter 1600: Meta Loss 0.5986 | Val Acc 0.2687 (Best: 0.3033)\n",
            "   Iter 1700: Meta Loss 0.5776 | Val Acc 0.2200 (Best: 0.3033)\n",
            "   Iter 1800: Meta Loss 0.6427 | Val Acc 0.2473 (Best: 0.3033)\n",
            "   Iter 1900: Meta Loss 0.6142 | Val Acc 0.2608 (Best: 0.3033)\n",
            "   Iter 1999: Meta Loss 0.6881 | Val Acc 0.2547 (Best: 0.3033)\n",
            "Training Complete. Best Val Acc: 0.3033\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_50\n",
            "========================================\n",
            "   [Gen] Filter: Using 3000 of 6000 classes (50.0%).\n",
            "   [Gen] Filter: Using 2000 of 2000 classes (100.0%).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4e9b82aef5d41e0be54fa36b44dd609",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Iter 0: Meta Loss 15.3077 | Val Acc 0.0570 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 100: Meta Loss 1.5342 | Val Acc 0.2247 (Best: 0.0570)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 200: Meta Loss 1.4327 | Val Acc 0.1777 (Best: 0.2247)\n",
            "   Iter 300: Meta Loss 1.2824 | Val Acc 0.2110 (Best: 0.2247)\n",
            "   Iter 400: Meta Loss 1.3728 | Val Acc 0.2152 (Best: 0.2247)\n",
            "   Iter 500: Meta Loss 1.3497 | Val Acc 0.1928 (Best: 0.2247)\n",
            "   Iter 600: Meta Loss 1.1273 | Val Acc 0.2027 (Best: 0.2247)\n",
            "   Iter 700: Meta Loss 1.0102 | Val Acc 0.2003 (Best: 0.2247)\n",
            "   Iter 800: Meta Loss 1.0793 | Val Acc 0.2268 (Best: 0.2247)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 900: Meta Loss 0.9976 | Val Acc 0.2262 (Best: 0.2268)\n",
            "   Iter 1000: Meta Loss 0.9117 | Val Acc 0.2417 (Best: 0.2268)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 1100: Meta Loss 1.0333 | Val Acc 0.2618 (Best: 0.2417)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 1200: Meta Loss 0.8805 | Val Acc 0.2312 (Best: 0.2618)\n",
            "   Iter 1300: Meta Loss 0.9102 | Val Acc 0.2565 (Best: 0.2618)\n",
            "   Iter 1400: Meta Loss 0.8661 | Val Acc 0.2268 (Best: 0.2618)\n",
            "   Iter 1500: Meta Loss 0.8672 | Val Acc 0.2568 (Best: 0.2618)\n",
            "   Iter 1600: Meta Loss 0.7010 | Val Acc 0.2625 (Best: 0.2618)\n",
            "   [New Best] Saved M_maml_50_best.pth\n",
            "   Iter 1700: Meta Loss 0.7686 | Val Acc 0.2292 (Best: 0.2625)\n",
            "   Iter 1800: Meta Loss 0.7112 | Val Acc 0.2227 (Best: 0.2625)\n",
            "   Iter 1900: Meta Loss 0.8534 | Val Acc 0.2285 (Best: 0.2625)\n",
            "   Iter 1999: Meta Loss 0.6526 | Val Acc 0.2327 (Best: 0.2625)\n",
            "Training Complete. Best Val Acc: 0.2625\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_100\n",
            "========================================\n",
            "   [Gen] Filter: Using 6000 of 6000 classes (100.0%).\n",
            "   [Gen] Filter: Using 2000 of 2000 classes (100.0%).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67e3a7bbbd0040eb893ac01b7929e60e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Iter 0: Meta Loss 16.5502 | Val Acc 0.0253 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_100_best.pth\n",
            "   Iter 100: Meta Loss 1.1836 | Val Acc 0.2645 (Best: 0.0253)\n",
            "   [New Best] Saved M_maml_100_best.pth\n",
            "   Iter 200: Meta Loss 1.2588 | Val Acc 0.2608 (Best: 0.2645)\n",
            "   Iter 300: Meta Loss 1.1199 | Val Acc 0.2547 (Best: 0.2645)\n",
            "   Iter 400: Meta Loss 1.1043 | Val Acc 0.2005 (Best: 0.2645)\n",
            "   Iter 500: Meta Loss 1.0065 | Val Acc 0.0782 (Best: 0.2645)\n",
            "   Iter 600: Meta Loss 0.9885 | Val Acc 0.2233 (Best: 0.2645)\n",
            "   Iter 700: Meta Loss 1.0095 | Val Acc 0.2173 (Best: 0.2645)\n",
            "   Iter 800: Meta Loss 0.9849 | Val Acc 0.2135 (Best: 0.2645)\n",
            "   Iter 900: Meta Loss 0.9621 | Val Acc 0.2320 (Best: 0.2645)\n",
            "   Iter 1000: Meta Loss 0.8791 | Val Acc 0.2188 (Best: 0.2645)\n",
            "   Iter 1100: Meta Loss 0.8400 | Val Acc 0.2145 (Best: 0.2645)\n",
            "   Iter 1200: Meta Loss 0.5838 | Val Acc 0.2508 (Best: 0.2645)\n",
            "   Iter 1300: Meta Loss 0.6693 | Val Acc 0.2417 (Best: 0.2645)\n",
            "   Iter 1400: Meta Loss 0.8456 | Val Acc 0.2048 (Best: 0.2645)\n",
            "   Iter 1500: Meta Loss 0.7860 | Val Acc 0.2773 (Best: 0.2645)\n",
            "   [New Best] Saved M_maml_100_best.pth\n",
            "   Iter 1600: Meta Loss 0.7724 | Val Acc 0.2135 (Best: 0.2773)\n",
            "   Iter 1700: Meta Loss 0.6699 | Val Acc 0.2527 (Best: 0.2773)\n",
            "   Iter 1800: Meta Loss 0.8011 | Val Acc 0.2517 (Best: 0.2773)\n",
            "   Iter 1900: Meta Loss 0.6800 | Val Acc 0.2325 (Best: 0.2773)\n",
            "   Iter 1999: Meta Loss 0.5556 | Val Acc 0.2707 (Best: 0.2773)\n",
            "Training Complete. Best Val Acc: 0.2773\n",
            "\n",
            "PHASE 5 COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 5.2: HIGH-PERFORMANCE TUNING (A100 OPTIMIZED)\n",
        "#################################################################\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"\\n--- PHASE 5.2: High-Performance Tuning ---\")\n",
        "\n",
        "# --- 1. OPTIMIZED DATA GENERATOR ---\n",
        "# Wir erweitern den Generator, um 'Chunks' von Tasks zu liefern\n",
        "class A100TaskGenerator:\n",
        "    def __init__(self, dataset, ways, shots, query_shots):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Caching indices for speed\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl not in self.indices_by_label: self.indices_by_label[lbl] = []\n",
        "            self.indices_by_label[lbl].append(idx)\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        \"\"\"Generates a whole BATCH of tasks at once to minimize CPU overhead.\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            selected_classes = random.sample(self.classes, self.ways)\n",
        "            task_imgs, task_lbls = [], []\n",
        "\n",
        "            for local_label, global_label in enumerate(selected_classes):\n",
        "                indices = self.indices_by_label[global_label]\n",
        "                needed = self.shots + self.query_shots\n",
        "                # Fast sampling\n",
        "                selected = random.sample(indices, needed) if len(indices) >= needed else random.choices(indices, k=needed)\n",
        "\n",
        "                for idx in selected:\n",
        "                    img, _ = self.dataset[idx]\n",
        "                    task_imgs.append(img)\n",
        "                    task_lbls.append(local_label)\n",
        "\n",
        "            # Stack images for this task: [N_Samples, C, H, W]\n",
        "            all_data.append(torch.stack(task_imgs))\n",
        "            all_labels.append(torch.tensor(task_lbls))\n",
        "\n",
        "        # Return stacked meta-batch: [MetaBatch, N_Samples, C, H, W]\n",
        "        return torch.stack(all_data), torch.stack(all_labels)\n",
        "\n",
        "# Wrapper to create the optimized generator\n",
        "def get_fast_taskset(split, ways=5, shots=5, query_shots=15):\n",
        "    # Re-use existing MetaINatDataset logic\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((84, 84)), # MAML standard size (faster)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "    ds = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=train_transforms)\n",
        "    return A100TaskGenerator(ds, ways, shots, query_shots)\n",
        "\n",
        "\n",
        "# --- 2. OPTIMIZED STEP FUNCTION ---\n",
        "def tuning_step_optimized(meta_model, task_generator, meta_optimizer, criterion, device, current_cfg, meta_batch_size):\n",
        "    meta_loss_total = 0.0\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    # 1. FETCH ALL DATA AT ONCE (CPU -> GPU Transfer happens ONCE)\n",
        "    tasks_data, tasks_labels = task_generator.sample_batch(batch_size=meta_batch_size)\n",
        "    tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "\n",
        "    # Pre-calculate indices\n",
        "    ways, shots, queries = 5, 5, 15\n",
        "    support_indices = []\n",
        "    query_indices = []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    # 2. LOOP ON GPU (No more CPU waiting)\n",
        "    for i in range(meta_batch_size):\n",
        "        # Slicing on GPU is fast\n",
        "        supp_X = tasks_data[i][support_indices]\n",
        "        supp_y = tasks_labels[i][support_indices]\n",
        "        query_X = tasks_data[i][query_indices]\n",
        "        query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "        # --- Standard MAML Inner Loop ---\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train()\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=current_cfg['INNER_LR'])\n",
        "\n",
        "        for _ in range(current_cfg['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        q_preds = fast_model(query_X)\n",
        "        q_loss = criterion(q_preds, query_y)\n",
        "        meta_loss_total += q_loss.item()\n",
        "        q_loss.backward()\n",
        "\n",
        "        # Accumulate Gradients\n",
        "        for mp, fp in zip(meta_model.parameters(), fast_model.parameters()):\n",
        "            if fp.grad is not None:\n",
        "                grad = fp.grad.detach() / meta_batch_size\n",
        "                if mp.grad is None: mp.grad = grad\n",
        "                else: mp.grad += grad\n",
        "\n",
        "        del fast_model, inner_opt # Free VRAM immediately\n",
        "\n",
        "    meta_optimizer.step()\n",
        "    return meta_loss_total / meta_batch_size\n",
        "\n",
        "\n",
        "# --- 3. TUNING CONFIGURATION ---\n",
        "GRID_SEARCH = {\n",
        "    'META_LR': [1e-3, 1e-4],\n",
        "    'INNER_LR': [0.1, 0.01],\n",
        "    'INNER_STEPS': [1, 5]\n",
        "}\n",
        "\n",
        "TUNING_CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "    'SUBSET': 0.25,\n",
        "    'ITERS': 100,       # Weniger Iterationen, da Batch Size größer!\n",
        "    'META_BATCH': 32,   # <--- HIER IST DER TURBO! (Standard war 4)\n",
        "    'VAL_INT': 25,\n",
        "    'DRY_RUN': True\n",
        "}\n",
        "\n",
        "if TUNING_CONFIG['DRY_RUN']:\n",
        "    TUNING_CONFIG['ITERS'] = 2\n",
        "    TUNING_CONFIG['VAL_INT'] = 1\n",
        "    TUNING_CONFIG['META_BATCH'] = 2\n",
        "\n",
        "\n",
        "# --- 4. RUN ENGINE ---\n",
        "def run_tuning(combo_params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    meta_model = load_base_model_for_maml(TUNING_CONFIG['SUBSET'], TUNING_CONFIG['ARCH'])\n",
        "    meta_model = meta_model.to(device)\n",
        "    meta_optimizer = optim.Adam(meta_model.parameters(), lr=combo_params['META_LR'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use optimized generator\n",
        "    train_gen = get_fast_taskset(split='c_base')\n",
        "    val_gen = get_fast_taskset(split='c_val') # Reuse for simplicity\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    pbar = tqdm(range(TUNING_CONFIG['ITERS']), desc=\"Run\", leave=False)\n",
        "    for i in pbar:\n",
        "        loss = tuning_step_optimized(meta_model, train_gen, meta_optimizer, criterion, device, combo_params, TUNING_CONFIG['META_BATCH'])\n",
        "        pbar.set_postfix(loss=f\"{loss:.3f}\")\n",
        "\n",
        "        if (i+1) % TUNING_CONFIG['VAL_INT'] == 0:\n",
        "            # Quick Validation (Zero-Shot Proxy for speed)\n",
        "            meta_model.eval()\n",
        "            correct, total = 0, 0\n",
        "            # Validate on 1 batch of tasks\n",
        "            v_data, v_lbl = val_gen.sample_batch(4)\n",
        "            v_data, v_lbl = v_data.to(device), v_lbl.to(device)\n",
        "            with torch.no_grad():\n",
        "                # Flatten batch for simple forward pass check\n",
        "                B, N, C, H, W = v_data.shape\n",
        "                logits = meta_model(v_data.view(-1, C, H, W))\n",
        "                targets = v_lbl.view(-1)\n",
        "                _, preds = torch.max(logits, 1)\n",
        "                correct += (preds == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "            acc = correct / total\n",
        "            if acc > best_acc: best_acc = acc\n",
        "            meta_model.train()\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "print(f\"Starting A100 Optimized Tuning (Batch Size {TUNING_CONFIG['META_BATCH']})...\")\n",
        "results = []\n",
        "keys = list(GRID_SEARCH.keys())\n",
        "combos = list(itertools.product(*GRID_SEARCH.values()))\n",
        "\n",
        "combo_pbar = tqdm(combos, desc=\"Total Progress\")\n",
        "for c in combo_pbar:\n",
        "    params = dict(zip(keys, c))\n",
        "    try:\n",
        "        score = run_tuning(params)\n",
        "        res = params.copy()\n",
        "        res['Acc'] = score\n",
        "        results.append(res)\n",
        "        combo_pbar.write(f\" -> {params} | Acc: {score:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# --- 6. RESULTS ---\n",
        "if results:\n",
        "    df = pd.DataFrame(results).sort_values(by='Acc', ascending=False)\n",
        "    print(\"\\n\", df)\n",
        "\n",
        "    # Heatmap logic\n",
        "    unique_steps = df['INNER_STEPS'].unique()\n",
        "    fig, axes = plt.subplots(1, len(unique_steps), figsize=(12, 5), sharey=True)\n",
        "    if len(unique_steps) == 1: axes = [axes]\n",
        "    for i, steps in enumerate(sorted(unique_steps)):\n",
        "        ax = axes[i]\n",
        "        subset = df[df['INNER_STEPS'] == steps]\n",
        "        pivot = subset.pivot(index='META_LR', columns='INNER_LR', values='Acc')\n",
        "        sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"viridis\", ax=ax)\n",
        "        ax.set_title(f\"Inner Steps: {steps}\")\n",
        "    plt.show()\n",
        "\n",
        "    win = df.iloc[0]\n",
        "    print(f\"\\nWINNER: MetaLR={win['META_LR']}, InnerLR={win['INNER_LR']}, Steps={int(win['INNER_STEPS'])}\")"
      ],
      "metadata": {
        "id": "W3hgf4PdMhLb"
      },
      "id": "W3hgf4PdMhLb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e1b01f947c5465f9d6d7075fad8c4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ab8a89c0d454c76827134212a9a8a92",
              "IPY_MODEL_3457a26430f4421e800a450ef281a0cb",
              "IPY_MODEL_407e9cbc55584baaa38fa60c71ec73b6"
            ],
            "layout": "IPY_MODEL_eeed7935da014b539b3ccc381d7573da"
          }
        },
        "5ab8a89c0d454c76827134212a9a8a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26e83b8fd5494051aeda3038e8cc8cea",
            "placeholder": "​",
            "style": "IPY_MODEL_c1e01440edad449bb304cf05a684c541",
            "value": "Caching: 100%"
          }
        },
        "3457a26430f4421e800a450ef281a0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ee388c999b42bbaa77844a38840414",
            "max": 300000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19ee9ef2e4d04e56a8fced0a5654f137",
            "value": 300000
          }
        },
        "407e9cbc55584baaa38fa60c71ec73b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc16b6ec11374322b98a05a49d480d51",
            "placeholder": "​",
            "style": "IPY_MODEL_078016825940482b807bdb217a1477a0",
            "value": " 300000/300000 [22:01&lt;00:00, 197.13it/s]"
          }
        },
        "eeed7935da014b539b3ccc381d7573da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e83b8fd5494051aeda3038e8cc8cea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e01440edad449bb304cf05a684c541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7ee388c999b42bbaa77844a38840414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19ee9ef2e4d04e56a8fced0a5654f137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc16b6ec11374322b98a05a49d480d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078016825940482b807bdb217a1477a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc5a5c455d09406c814632f4274a3da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54064146be704ceea0a819cc9639ad36",
              "IPY_MODEL_37816b20beaa4a23a1f20cbc166440d4",
              "IPY_MODEL_6b2a8c1fb9b3433097092d4b8940c943"
            ],
            "layout": "IPY_MODEL_42422a3bbda4494c8de0e6d36b26acfe"
          }
        },
        "54064146be704ceea0a819cc9639ad36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abe45d90f4543d8b7224f5926f962f3",
            "placeholder": "​",
            "style": "IPY_MODEL_d7da4f2f1b9648799d926fefd935fa68",
            "value": "Caching: 100%"
          }
        },
        "37816b20beaa4a23a1f20cbc166440d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2552765998a245debb5404f06e8b35fa",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0792ad57e2bb464f918aac6c789a59df",
            "value": 100000
          }
        },
        "6b2a8c1fb9b3433097092d4b8940c943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a1c46a880864aa2a65afc9ab6dbd53a",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a0f4a3eb5b4387b9d9202eb53aa39c",
            "value": " 100000/100000 [08:32&lt;00:00, 186.99it/s]"
          }
        },
        "42422a3bbda4494c8de0e6d36b26acfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abe45d90f4543d8b7224f5926f962f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7da4f2f1b9648799d926fefd935fa68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2552765998a245debb5404f06e8b35fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0792ad57e2bb464f918aac6c789a59df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a1c46a880864aa2a65afc9ab6dbd53a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a0f4a3eb5b4387b9d9202eb53aa39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "158b12ae8fc34e92ae5c21a8c1837594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0723d19165c84f5198f4f98beb0a84a8",
              "IPY_MODEL_a6fa8bb0281a42e68993f968020286bf",
              "IPY_MODEL_23e4f2f39d3b43bbbab5f6816fc64243"
            ],
            "layout": "IPY_MODEL_9b04a012e7a641b8bf8d2aa348558e8a"
          }
        },
        "0723d19165c84f5198f4f98beb0a84a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29bfc92b4c6244eabba7f709363caee5",
            "placeholder": "​",
            "style": "IPY_MODEL_ae52634b8a3c48c5bf1ade4bcec712a8",
            "value": "Training: 100%"
          }
        },
        "a6fa8bb0281a42e68993f968020286bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2ddc790ce54837831f6da0dcada2bd",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4349e57318384b0b81bd082a91611bee",
            "value": 2000
          }
        },
        "23e4f2f39d3b43bbbab5f6816fc64243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e557682deef45818fba08509d2501a6",
            "placeholder": "​",
            "style": "IPY_MODEL_4e9dfc2a5b864c0ca9d9a1a65b00c5b7",
            "value": " 2000/2000 [1:49:39&lt;00:00,  7.73s/it, loss=0.6881]"
          }
        },
        "9b04a012e7a641b8bf8d2aa348558e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29bfc92b4c6244eabba7f709363caee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae52634b8a3c48c5bf1ade4bcec712a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2ddc790ce54837831f6da0dcada2bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4349e57318384b0b81bd082a91611bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e557682deef45818fba08509d2501a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9dfc2a5b864c0ca9d9a1a65b00c5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4e9b82aef5d41e0be54fa36b44dd609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b461d8776fb044039a062a2fbc9f5c5a",
              "IPY_MODEL_ddc1a725915b45cf8412309b48b1dcf2",
              "IPY_MODEL_481242dd252a44d3a0eba7263db05645"
            ],
            "layout": "IPY_MODEL_5ae9aeb800e141518ab3332f7a1dfab9"
          }
        },
        "b461d8776fb044039a062a2fbc9f5c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40dceae64d6f4fe1b9693937258931cb",
            "placeholder": "​",
            "style": "IPY_MODEL_67dfc263182b4fbe9ab83429949b2b3b",
            "value": "Training:  96%"
          }
        },
        "ddc1a725915b45cf8412309b48b1dcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98bde641b6b24edc974b748555634d89",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d2a762892ad40dab1052349ce0d94c6",
            "value": 1918
          }
        },
        "481242dd252a44d3a0eba7263db05645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ddfc54c8daf4b74be14cf38dad7311b",
            "placeholder": "​",
            "style": "IPY_MODEL_cab7e6e419204920a3f8d647e880d7b3",
            "value": " 1918/2000 [1:44:12&lt;04:12,  3.08s/it, loss=0.7557]"
          }
        },
        "5ae9aeb800e141518ab3332f7a1dfab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40dceae64d6f4fe1b9693937258931cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67dfc263182b4fbe9ab83429949b2b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98bde641b6b24edc974b748555634d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2a762892ad40dab1052349ce0d94c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ddfc54c8daf4b74be14cf38dad7311b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab7e6e419204920a3f8d647e880d7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}