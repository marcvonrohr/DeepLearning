{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcvonrohr/DeepLearning/blob/main/meta_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54242527",
      "metadata": {
        "id": "54242527",
        "outputId": "53000f13-617e-44d8-b2e8-445c0db4371d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting Google Drive...\n",
            "Mounted at /content/drive\n",
            "...Google Drive connected.\n",
            "Local data directory created at: /content/data/inaturalist_unpacked\n",
            "\n",
            "--- Processing 2021_train_mini ---\n",
            "Copying '2021_train_mini.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 927.91 seconds.\n",
            "Unpacking '2021_train_mini.tar.gz' locally...\n",
            "...Unpacking complete. Took 438.16 seconds.\n",
            "Deleting local tarball '/content/data/2021_train_mini.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Processing 2021_valid ---\n",
            "Copying '2021_valid.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 204.09 seconds.\n",
            "Unpacking '2021_valid.tar.gz' locally...\n",
            "...Unpacking complete. Took 77.49 seconds.\n",
            "Deleting local tarball '/content/data/2021_valid.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Final Data Setup Verification ---\n",
            "Dataset is ready for training at: /content/data/inaturalist_unpacked\n",
            "total 2.5M\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 train_mini\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 val\n",
            "\n",
            "Local VM Disk Space Usage:\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   98G  139G  42% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              41G     0   41G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "/dev/sda1       242G  143G  100G  60% /kaggle/input\n",
            "tmpfs            42G  116K   42G   1% /var/colab\n",
            "tmpfs            42G     0   42G   0% /proc/acpi\n",
            "tmpfs            42G     0   42G   0% /proc/scsi\n",
            "tmpfs            42G     0   42G   0% /sys/firmware\n",
            "drive           236G  105G  132G  45% /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "#################################################################\n",
        "#  STEP 2.1: PREPARE LOCAL VM\n",
        "#################################################################\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "print(\"Connecting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"...Google Drive connected.\")\n",
        "\n",
        "# --- 2. Define Key Paths ---\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "DATASETS_ROOT_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
        "INAT_ROOT_DIR = os.path.join(DATASETS_ROOT_DIR, 'inaturalist')\n",
        "\n",
        "# Source: The COMPRESSED archives\n",
        "ARCHIVES_DIR_ON_DRIVE = os.path.join(INAT_ROOT_DIR, 'archives')\n",
        "\n",
        "# Target: The LOCAL VM fast disk\n",
        "LOCAL_DATA_ROOT = '/content/data'\n",
        "# This is the final path your PyTorch code will use:\n",
        "FINAL_DATA_PATH = os.path.join(LOCAL_DATA_ROOT, 'inaturalist_unpacked')\n",
        "\n",
        "# Define source/destination paths\n",
        "TAR_FILES = {\n",
        "    \"2021_train_mini\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_train_mini.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_train_mini.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_train_mini')\n",
        "    },\n",
        "    \"2021_valid\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_valid.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_valid.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_valid')\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 3. Create Local Directories on VM ---\n",
        "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(FINAL_DATA_PATH, exist_ok=True)\n",
        "print(f\"Local data directory created at: {FINAL_DATA_PATH}\")\n",
        "\n",
        "# --- 4. Copy, Unpack, and Clean up for each file ---\n",
        "for name, paths in TAR_FILES.items():\n",
        "    print(f\"\\n--- Processing {name} ---\")\n",
        "\n",
        "    if os.path.exists(paths[\"check_unpacked\"]):\n",
        "        print(f\"'{name}' is already unpacked in local VM. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 4a. Copy .tar.gz from Drive to local VM\n",
        "    print(f\"Copying '{name}.tar.gz' from Drive to local VM...\")\n",
        "    start_time = time.time()\n",
        "    !cp \"{paths['src']}\" \"{paths['dest_tar']}\"\n",
        "    print(f\"...Copy complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4b. Unpack the file on the local VM\n",
        "    print(f\"Unpacking '{name}.tar.gz' locally...\")\n",
        "    start_time = time.time()\n",
        "    !tar -xzf \"{paths['dest_tar']}\" -C \"{FINAL_DATA_PATH}\"\n",
        "    print(f\"...Unpacking complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4c. Delete the local .tar.gz file to save VM space\n",
        "    print(f\"Deleting local tarball '{paths['dest_tar']}'...\")\n",
        "    !rm \"{paths['dest_tar']}\"\n",
        "    print(\"...Local tarball deleted.\")\n",
        "\n",
        "# --- 5. Verify and Set Path for Training ---\n",
        "print(\"\\n--- Final Data Setup Verification ---\")\n",
        "print(f\"Dataset is ready for training at: {FINAL_DATA_PATH}\")\n",
        "!ls -lh \"{FINAL_DATA_PATH}\"\n",
        "print(\"\\nLocal VM Disk Space Usage:\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.2: SCIENTIFIC DATA PARTITIONING\n",
        "#################################################################\n",
        "print(\"\\n--- STEP 2.2: Loading/Creating Scientific Class Partition ---\")\n",
        "\n",
        "# --- 6. Define Paths for Partition File ---\n",
        "# We create a 'project_meta' folder on GDrive to store helper files\n",
        "META_DIR_ON_DRIVE = os.path.join(PROJECT_DIR, 'project_meta')\n",
        "os.makedirs(META_DIR_ON_DRIVE, exist_ok=True)\n",
        "\n",
        "PARTITION_FILE_PATH = os.path.join(META_DIR_ON_DRIVE, 'inat_class_split.json')\n",
        "print(f\"Looking for partition file at: {PARTITION_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_ARRY2GUvXt",
        "outputId": "59831a02-8bd6-4103-cac1-c8d25751d314"
      },
      "id": "1_ARRY2GUvXt",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.2: Loading/Creating Scientific Class Partition ---\n",
            "Looking for partition file at: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Logic to Find Classes and Create Partition ---\n",
        "\n",
        "# 7a. Identify the Dataset Root\n",
        "# The unpacking might have created a subfolder (e.g., '2021_train_mini' or 'train_mini')\n",
        "# or files might be directly in FINAL_DATA_PATH. We check common patterns.\n",
        "possible_roots = [\n",
        "    os.path.join(FINAL_DATA_PATH, '2021_train_mini'),\n",
        "    os.path.join(FINAL_DATA_PATH, 'train_mini'),\n",
        "    FINAL_DATA_PATH\n",
        "]\n",
        "\n",
        "DATASET_ROOT = None\n",
        "for path in possible_roots:\n",
        "    if os.path.exists(path):\n",
        "        # Check if this path actually contains subdirectories\n",
        "        if len([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]) > 0:\n",
        "            DATASET_ROOT = path\n",
        "            break\n",
        "\n",
        "print(f\"Dataset root identified as: {DATASET_ROOT}\")\n",
        "\n",
        "# 7b. Load or Create the Partition\n",
        "partition_data = {}\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "if os.path.exists(PARTITION_FILE_PATH):\n",
        "    print(\"Found existing partition file. Loading...\")\n",
        "    with open(PARTITION_FILE_PATH, 'r') as f:\n",
        "        partition_data = json.load(f)\n",
        "else:\n",
        "    print(\"No partition file found. Scanning directories to create new partition...\")\n",
        "    print(\"This ensures independence from missing metadata files.\")\n",
        "\n",
        "    # --- Scan for Class Folders ---\n",
        "    class_folders_rel = []\n",
        "\n",
        "    # Walk through the directory tree\n",
        "    # A \"class\" is any folder that contains image files (.jpg, .jpeg, .png)\n",
        "    print(\"Scanning folders (this may take 1-2 minutes)...\")\n",
        "    for root, dirs, files in os.walk(DATASET_ROOT):\n",
        "        # Check for images in this specific folder\n",
        "        images = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if len(images) > 0:\n",
        "            # Get path relative to the dataset root (e.g., \"Aves/Turdus_migratorius\")\n",
        "            rel_path = os.path.relpath(root, DATASET_ROOT)\n",
        "            class_folders_rel.append(rel_path)\n",
        "\n",
        "    # --- CRITICAL: Sort for Reproducibility ---\n",
        "    # Sorting ensures that Index 0 is ALWAYS the same class on every machine/run\n",
        "    class_folders_rel.sort()\n",
        "\n",
        "    num_classes = len(class_folders_rel)\n",
        "    print(f\"Found {num_classes} classes containing images.\")\n",
        "\n",
        "    if num_classes < 9900:\n",
        "        print(\"WARNING: Found significantly fewer than 10,000 classes. Check extraction.\")\n",
        "\n",
        "    # --- Assign IDs and Shuffle ---\n",
        "    all_class_ids = list(range(num_classes))\n",
        "\n",
        "    print(f\"Shuffling {num_classes} class IDs with random seed {RANDOM_SEED}...\")\n",
        "    random.seed(RANDOM_SEED)\n",
        "    random.shuffle(all_class_ids)\n",
        "\n",
        "    # --- Split into Sets ---\n",
        "    # 6000 Base (Train/Meta-Train), 2000 Val (Hyperparams), 2000 Novel (Test)\n",
        "    c_base_ids = all_class_ids[:6000]\n",
        "    c_val_ids = all_class_ids[6000:8000]\n",
        "    c_novel_ids = all_class_ids[8000:]\n",
        "\n",
        "    # --- Construct Data Structure ---\n",
        "    # We save both the sets AND the mapping from ID -> Folder Path\n",
        "    partition_data = {\n",
        "        \"sets\": {\n",
        "            'c_base': sorted(c_base_ids),\n",
        "            'c_val': sorted(c_val_ids),\n",
        "            'c_novel': sorted(c_novel_ids)\n",
        "        },\n",
        "        \"id_to_path\": {\n",
        "            str(i): folder_path for i, folder_path in enumerate(class_folders_rel)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    print(f\"Saving new partition and mapping to: {PARTITION_FILE_PATH}\")\n",
        "    with open(PARTITION_FILE_PATH, 'w') as f:\n",
        "        json.dump(partition_data, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987lbsNFU4Qf",
        "outputId": "64586ee8-0d48-49b4-f047-f2db353d4cd8"
      },
      "id": "987lbsNFU4Qf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset root identified as: /content/data/inaturalist_unpacked/train_mini\n",
            "Found existing partition file. Loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Verification ---\n",
        "print(\"\\n--- Partitioning Complete ---\")\n",
        "sets = partition_data['sets']\n",
        "print(f\"Total C_base classes:  {len(sets['c_base'])}\")\n",
        "print(f\"Total C_val classes:   {len(sets['c_val'])}\")\n",
        "print(f\"Total C_novel classes: {len(sets['c_novel'])}\")\n",
        "\n",
        "# Check for overlaps (should be 0)\n",
        "base_set = set(sets['c_base'])\n",
        "val_set = set(sets['c_val'])\n",
        "novel_set = set(sets['c_novel'])\n",
        "\n",
        "overlap_bv = base_set & val_set\n",
        "overlap_bn = base_set & novel_set\n",
        "overlap_vn = val_set & novel_set\n",
        "\n",
        "print(f\"Overlap (Base-Val):    {len(overlap_bv)}\")\n",
        "print(f\"Overlap (Base-Novel):  {len(overlap_bn)}\")\n",
        "print(f\"Overlap (Val-Novel):   {len(overlap_vn)}\")\n",
        "\n",
        "if len(overlap_bv) + len(overlap_bn) + len(overlap_vn) == 0:\n",
        "    print(\"\\nSUCCESS: Classes are cleanly partitioned.\")\n",
        "else:\n",
        "    print(\"\\nCRITICAL ERROR: Overlaps detected in class sets!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8xIL6YPU6OM",
        "outputId": "e4d17c4a-264f-4543-9689-9569ba22c0c2"
      },
      "id": "w8xIL6YPU6OM",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Partitioning Complete ---\n",
            "Total C_base classes:  6000\n",
            "Total C_val classes:   2000\n",
            "Total C_novel classes: 2000\n",
            "Overlap (Base-Val):    0\n",
            "Overlap (Base-Novel):  0\n",
            "Overlap (Val-Novel):   0\n",
            "\n",
            "SUCCESS: Classes are cleanly partitioned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.3: MODULAR DATA LOADERS (NO LEARN2LEARN DEPENDENCY)\n",
        "#################################################################\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "print(\"\\n--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\")\n",
        "\n",
        "# --- SAFETY CHECK ---\n",
        "# Ensure variables from Step 2.2 exist\n",
        "required_vars = ['DATASET_ROOT', 'PARTITION_FILE_PATH']\n",
        "if not all(v in globals() for v in required_vars):\n",
        "    raise NameError(f\"Missing variables from Step 2.2. Please run the previous cell.\")\n",
        "\n",
        "print(f\"Using Dataset Root: {DATASET_ROOT}\")\n",
        "print(f\"Using Partition File: {PARTITION_FILE_PATH}\")\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZE_STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ7TNDctB_kt",
        "outputId": "c1f8efbd-7bf1-497d-add9-35cdf3f3d215"
      },
      "id": "ZZ7TNDctB_kt",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\n",
            "Using Dataset Root: /content/data/inaturalist_unpacked/train_mini\n",
            "Using Partition File: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  CORE COMPONENT: The Custom Dataset Class\n",
        "# ==============================================================================\n",
        "class MetaINatDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset that enforces the scientific partition.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, partition_file, split='c_base', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        with open(partition_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if split not in data['sets']:\n",
        "            raise ValueError(f\"Invalid split '{split}'. Available: {list(data['sets'].keys())}\")\n",
        "\n",
        "        self.allowed_ids = data['sets'][split]\n",
        "        self.id_to_path = data['id_to_path']\n",
        "\n",
        "        # Map original ID -> 0..N-1\n",
        "        self.label_map = {orig: new for new, orig in enumerate(self.allowed_ids)}\n",
        "\n",
        "        self.samples = []\n",
        "        for original_id in self.allowed_ids:\n",
        "            rel_path = self.id_to_path[str(original_id)]\n",
        "            abs_path = os.path.join(self.root_dir, rel_path)\n",
        "            if os.path.exists(abs_path):\n",
        "                for img in os.listdir(abs_path):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        self.samples.append({\n",
        "                            'path': os.path.join(abs_path, img),\n",
        "                            'label': self.label_map[original_id]\n",
        "                        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = Image.open(sample['path']).convert('RGB')\n",
        "        label = sample['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ykdP8nDFcU6i"
      },
      "id": "ykdP8nDFcU6i",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  HELPER: Episodic Batch Generator (Replaces learn2learn)\n",
        "# ==============================================================================\n",
        "class EpisodicTaskGenerator:\n",
        "    \"\"\"\n",
        "    Native PyTorch implementation of an N-Way K-Shot task sampler.\n",
        "    Replaces learn2learn functionality without installation issues.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, ways, shots, query_shots):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Group all image indices by their label for fast sampling\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl not in self.indices_by_label:\n",
        "                self.indices_by_label[lbl] = []\n",
        "            self.indices_by_label[lbl].append(idx)\n",
        "\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # 1. Sample N random classes (Ways)\n",
        "        selected_classes = random.sample(self.classes, self.ways)\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # 2. Sample K + Q images from each class\n",
        "        for local_label, global_label_idx in enumerate(selected_classes):\n",
        "            indices = self.indices_by_label[global_label_idx]\n",
        "\n",
        "            # Ensure we have enough images, otherwise sample with replacement\n",
        "            needed = self.shots + self.query_shots\n",
        "            if len(indices) >= needed:\n",
        "                selected_indices = random.sample(indices, needed)\n",
        "            else:\n",
        "                selected_indices = random.choices(indices, k=needed)\n",
        "\n",
        "            # 3. Load images and re-label them to 0..N-1 for the episode\n",
        "            for idx in selected_indices:\n",
        "                img, _ = self.dataset[idx] # dataset returns (img, global_label)\n",
        "                batch_images.append(img)\n",
        "                # Important: The label for the loss function must be 0..Ways-1\n",
        "                batch_labels.append(local_label)\n",
        "\n",
        "        # Stack into a single tensor: [Ways * (Shots+Query), C, H, W]\n",
        "        data = torch.stack(batch_images)\n",
        "        labels = torch.tensor(batch_labels)\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def sample(self):\n",
        "        # Compatibility method to look like learn2learn\n",
        "        return self.__next__()"
      ],
      "metadata": {
        "id": "asZAUP18cfqQ"
      },
      "id": "asZAUP18cfqQ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER A: Standard Pre-Training Loader\n",
        "# ==============================================================================\n",
        "def get_standard_loader(split='c_base', batch_size=64, shuffle=True):\n",
        "    print(f\"\\n[Loader A] Initializing Standard Loader for split '{split}'...\")\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=train_transforms)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\" -> {len(dataset)} total images.\")\n",
        "    print(f\" -> {len(dataset.allowed_ids)} classes.\")\n",
        "    return loader, len(dataset.allowed_ids)"
      ],
      "metadata": {
        "id": "oVtAtf9zcm9e"
      },
      "id": "oVtAtf9zcm9e",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER B: Episodic Task Loader (MAML) - NATIVE IMPLEMENTATION\n",
        "# ==============================================================================\n",
        "def get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1, img_size=84):\n",
        "    print(f\"\\n[Loader B] Initializing Episodic Generator for split '{split}'...\")\n",
        "\n",
        "    maml_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=maml_transforms)\n",
        "\n",
        "    # Use our native generator instead of learn2learn\n",
        "    task_generator = EpisodicTaskGenerator(\n",
        "        dataset,\n",
        "        ways=ways,\n",
        "        shots=shots,\n",
        "        query_shots=query_shots\n",
        "    )\n",
        "\n",
        "    print(f\" -> Configured {ways}-Way {shots}-Shot Tasks (Native PyTorch).\")\n",
        "    return task_generator"
      ],
      "metadata": {
        "id": "madMlUDOc5CK"
      },
      "id": "madMlUDOc5CK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER C: Fixed Few-Shot Loader for FT/LoRA\n",
        "# ==============================================================================\n",
        "def get_fixed_few_shot_task(split='c_novel', ways=5, shots=1, query_shots=15, seed=None):\n",
        "    print(f\"\\n[Loader C] Creating Fixed Few-Shot Task from '{split}'...\")\n",
        "\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    eval_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=eval_transforms)\n",
        "\n",
        "    available_labels = list(set(s['label'] for s in dataset.samples))\n",
        "    selected_classes = random.sample(available_labels, ways)\n",
        "\n",
        "    class_indices = {c: [] for c in selected_classes}\n",
        "    for idx, sample in enumerate(dataset.samples):\n",
        "        if sample['label'] in selected_classes:\n",
        "            class_indices[sample['label']].append(idx)\n",
        "\n",
        "    support_indices = []\n",
        "    query_indices = []\n",
        "\n",
        "    for c in selected_classes:\n",
        "        idxs = class_indices[c]\n",
        "        random.shuffle(idxs)\n",
        "        support_indices.extend(idxs[:shots])\n",
        "        query_indices.extend(idxs[shots : shots+query_shots])\n",
        "\n",
        "    support_loader = DataLoader(Subset(dataset, support_indices), batch_size=16, shuffle=True)\n",
        "    query_loader = DataLoader(Subset(dataset, query_indices), batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\" -> Support Set: {len(support_indices)} images, Query Set: {len(query_indices)} images\")\n",
        "    return support_loader, query_loader"
      ],
      "metadata": {
        "id": "G1m0luxjc5gB"
      },
      "id": "G1m0luxjc5gB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  VERIFICATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Testing Loaders ---\")\n",
        "\n",
        "# Test A\n",
        "try:\n",
        "    l_std, n_cls = get_standard_loader(split='c_base', batch_size=4)\n",
        "    print(\"Loader A (Standard) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader A Failed: {e}\")\n",
        "\n",
        "# Test B (Now using Native Generator)\n",
        "try:\n",
        "    task_gen = get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1)\n",
        "    batch_data, batch_labels = task_gen.sample()\n",
        "    # Expected shape: [Way*(Shot+Query), 3, 84, 84] -> [5*(1+1), 3, 84, 84] = [10, 3, 84, 84]\n",
        "    print(f\"Loader B (Episodic) check: OK. Batch shape: {batch_data.shape}\")\n",
        "    if batch_labels.max() >= 5:\n",
        "        print(\"WARNING: Labels not properly remapped to 0..N-1\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader B Failed: {e}\")\n",
        "\n",
        "# Test C\n",
        "try:\n",
        "    sup_dl, q_dl = get_fixed_few_shot_task(split='c_novel', ways=5, shots=5)\n",
        "    print(\"Loader C (Fixed) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader C Failed: {e}\")\n",
        "\n",
        "print(\"\\nStep 2.3 Complete (Dependencies Fixed).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zceGZ_SdEeE",
        "outputId": "721471ff-6b01-498b-ba54-5726aa715c34"
      },
      "id": "3zceGZ_SdEeE",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Loaders ---\n",
            "\n",
            "[Loader A] Initializing Standard Loader for split 'c_base'...\n",
            " -> 300000 total images.\n",
            " -> 6000 classes.\n",
            "Loader A (Standard) check: OK.\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 1-Shot Tasks (Native PyTorch).\n",
            "Loader B (Episodic) check: OK. Batch shape: torch.Size([10, 3, 84, 84])\n",
            "\n",
            "[Loader C] Creating Fixed Few-Shot Task from 'c_novel'...\n",
            " -> Support Set: 25 images, Query Set: 75 images\n",
            "Loader C (Fixed) check: OK.\n",
            "\n",
            "Step 2.3 Complete (Dependencies Fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 4: INTELLIGENT PRE-TRAINING (MAX PERF & MEMORY SAFE)\n",
        "#################################################################\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "import gc  # <--- WICHTIG für Garbage Collection\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\")\n",
        "\n",
        "# --- 0. DRIVE & PATH SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. SEED SETUP ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# --- 2. HARDWARE DETECTION (TUNED FOR A100) ---\n",
        "def get_optimal_config():\n",
        "    cpu_count = os.cpu_count()\n",
        "    optimal_workers = min(cpu_count, 8)\n",
        "    device_name = \"CPU\"\n",
        "    batch_size = 16\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_name = gpu_name\n",
        "        # --- TUNING ---\n",
        "        if \"A100\" in gpu_name:\n",
        "            batch_size = 512  # <--- Aggressiver für A100 (40GB VRAM erlaubt das locker)\n",
        "        elif \"T4\" in gpu_name:\n",
        "            batch_size = 128\n",
        "        else:\n",
        "            batch_size = 64\n",
        "    else:\n",
        "        print(\"WARNING: No GPU detected!\")\n",
        "\n",
        "    return device_name, batch_size, optimal_workers\n",
        "\n",
        "detected_device, auto_bs, auto_workers = get_optimal_config()\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "\n",
        "    # --- CONTROL CENTER ---\n",
        "    'DRY_RUN': False,            # <--- REAL TRAINING\n",
        "    'NUM_EPOCHS': 20,\n",
        "    # ----------------------\n",
        "\n",
        "    'BATCH_SIZE': auto_bs,\n",
        "    'NUM_WORKERS': auto_workers,\n",
        "    'DEVICE_NAME': detected_device,\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'PATIENCE': 5,\n",
        "    'SUBSETS': [0.25, 0.50, 1.0],\n",
        "\n",
        "    'CHECKPOINT_DIR_LOC': '/content/checkpoints',\n",
        "    'CHECKPOINT_DIR_DRIVE': MODELS_DIR\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG['CHECKPOINT_DIR_LOC'], exist_ok=True)\n",
        "\n",
        "print(f\"\\nSystem Configuration:\")\n",
        "print(f\" -> Hardware:    {CONFIG['DEVICE_NAME']}\")\n",
        "print(f\" -> Batch Size:  {CONFIG['BATCH_SIZE']} (Optimized)\")\n",
        "print(f\" -> Workers:     {CONFIG['NUM_WORKERS']}\")\n",
        "print(f\" -> Mode:        {'DRY RUN' if CONFIG['DRY_RUN'] else 'REAL TRAINING'}\")\n",
        "\n",
        "# --- 4. MEMORY CLEANUP HELPER (NEW) ---\n",
        "def cleanup_memory():\n",
        "    \"\"\"Forces Garbage Collection and clears GPU Cache.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # Optional: Print stats to verify\n",
        "    # print(f\"   [Mem] Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# --- 5. MODEL FACTORY ---\n",
        "def get_base_model(arch_name, num_classes, pretrained=True):\n",
        "    # Loading logic same as before\n",
        "    if arch_name == 'resnet34':\n",
        "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet18':\n",
        "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet50':\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    else:\n",
        "        raise ValueError(\"Arch not supported\")\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# --- 6. DATA LOADER HELPER ---\n",
        "def get_subset_loader(fraction):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    full_ds = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split='c_base', transform=train_transforms)\n",
        "\n",
        "    total_base_classes = len(full_ds.allowed_ids)\n",
        "    target_num = int(total_base_classes * fraction)\n",
        "    subset_ids = full_ds.allowed_ids[:target_num]\n",
        "\n",
        "    # Filter samples (Memory efficient filtering logic)\n",
        "    # We recreate the list to drop references to unused samples\n",
        "    new_samples = [s for s in full_ds.samples if s['label'] < target_num]\n",
        "    full_ds.samples = new_samples\n",
        "    full_ds.allowed_ids = subset_ids\n",
        "    full_ds.label_map = {orig: new for new, orig in enumerate(subset_ids)}\n",
        "\n",
        "    print(f\"\\n[Data] Subset {fraction*100}%: {len(new_samples)} images, {target_num} classes.\")\n",
        "\n",
        "    num_val = int(0.1 * len(full_ds))\n",
        "    train_ds, val_ds = random_split(full_ds, [len(full_ds)-num_val, num_val],\n",
        "                                    generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True,\n",
        "                              num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False,\n",
        "                            num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, target_num\n",
        "\n",
        "# --- 7. ROBUST CHECKPOINTING ---\n",
        "def safe_copy_to_drive(local_path, filename, max_retries=5):\n",
        "    drive_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], filename)\n",
        "    if not os.path.exists(CONFIG['CHECKPOINT_DIR_DRIVE']):\n",
        "        try: os.makedirs(CONFIG['CHECKPOINT_DIR_DRIVE'], exist_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            shutil.copy(local_path, drive_path)\n",
        "            if os.path.exists(drive_path) and os.path.getsize(drive_path) > 0:\n",
        "                print(f\"   -> Drive Copy: SUCCESS\")\n",
        "                return\n",
        "        except Exception as e:\n",
        "            wait_time = 3 * attempt\n",
        "            print(f\"   [Retry {attempt}] Copy failed ({e}). Waiting {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"   [CRITICAL ERROR] Failed to copy {filename} to Drive.\")\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(model.state_dict(), local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "# --- 8. TRAINING ENGINE ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, target_epochs, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    run_tag = \"_dryrun\" if CONFIG['DRY_RUN'] else \"\"\n",
        "    ckpt_filename = f\"{model_name}{run_tag}_checkpoint.pth\"\n",
        "    best_filename = f\"{model_name}{run_tag}_best.pth\"\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_acc = -1.0\n",
        "\n",
        "    # Resume Logic\n",
        "    drive_ckpt_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], ckpt_filename)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"\\n[RESUME] Found: {ckpt_filename}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(drive_ckpt_path, map_location=device)\n",
        "            saved_epoch = checkpoint['epoch']\n",
        "\n",
        "            if CONFIG['DRY_RUN']:\n",
        "                print(f\"   -> (Dry Run) Resetting loop despite found epoch {saved_epoch+1}.\")\n",
        "                start_epoch = 0\n",
        "                best_acc = checkpoint.get('best_acc', -1.0)\n",
        "            else:\n",
        "                if saved_epoch >= (target_epochs - 1):\n",
        "                    print(f\"   -> Fully trained ({saved_epoch+1} epochs). Skipping.\")\n",
        "                    return model\n",
        "                start_epoch = saved_epoch + 1\n",
        "                best_acc = checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            if 'scaler_state_dict' in checkpoint:\n",
        "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "            print(f\"   -> Resuming with Best Acc: {best_acc:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   [ERROR] Checkpoint corrupted ({e}). Fresh start.\")\n",
        "    else:\n",
        "        print(f\"\\n[START] Fresh start for {model_name}.\")\n",
        "\n",
        "    effective_epochs = 2 if CONFIG['DRY_RUN'] else target_epochs\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(start_epoch, effective_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{effective_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        limit_batches = 5 if CONFIG['DRY_RUN'] else None\n",
        "\n",
        "        pbar = tqdm(train_loader, leave=False, desc=\"Training\")\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if limit_batches and i >= limit_batches: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        iter_size = (limit_batches * CONFIG['BATCH_SIZE']) if limit_batches else len(train_loader.dataset)\n",
        "        if iter_size == 0: iter_size = 1\n",
        "        epoch_acc = running_corrects.double() / iter_size\n",
        "        epoch_loss = running_loss / iter_size\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        val_limit = 5 if CONFIG['DRY_RUN'] else None\n",
        "        val_count = 0\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            if val_limit and i >= val_limit: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "            val_count += inputs.size(0)\n",
        "        val_acc = val_corrects.double() / val_count if val_count > 0 else 0.0\n",
        "        print(f\"   Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save Checkpoint\n",
        "        full_state = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'best_acc': best_acc\n",
        "        }\n",
        "        save_checkpoint(full_state, ckpt_filename)\n",
        "\n",
        "        # Save Best Model logic\n",
        "        save_condition = False\n",
        "        if val_acc > best_acc: save_condition = True\n",
        "        elif CONFIG['DRY_RUN'] and val_acc >= best_acc: save_condition = True\n",
        "        elif best_acc == -1.0: save_condition = True\n",
        "\n",
        "        if save_condition:\n",
        "            best_acc = val_acc\n",
        "            save_best_model(model, best_filename)\n",
        "            print(f\"   [New Best] Saved {best_filename}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if not CONFIG['DRY_RUN'] and patience_counter >= CONFIG['PATIENCE']:\n",
        "            print(f\"   [Early Stopping] Reached patience limit.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training Finished. Final Best Acc: {best_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 9. EXECUTION LOOP (WITH CLEANUP) ---\n",
        "for fraction in CONFIG['SUBSETS']:\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    print(f\"\\n{'='*40}\\nRUN: {subset_name}\\n{'='*40}\")\n",
        "\n",
        "    train_dl, val_dl, num_cls = get_subset_loader(fraction)\n",
        "    model = get_base_model(CONFIG['ARCH'], num_classes=num_cls)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "    # Train\n",
        "    train_model(model, train_dl, val_dl, criterion, optimizer, lr_scheduler, CONFIG['NUM_EPOCHS'], subset_name)\n",
        "\n",
        "    # --- MEMORY CLEANUP ---\n",
        "    print(f\"   [Cleanup] Clearing GPU memory after {subset_name}...\")\n",
        "    del model\n",
        "    del optimizer\n",
        "    del criterion\n",
        "    del train_dl\n",
        "    del val_dl\n",
        "    cleanup_memory() # Call helper to force GC and Empty Cache\n",
        "    print(f\"   [Cleanup] Done. Ready for next model.\\n\")\n",
        "\n",
        "print(\"\\nPHASE 4 COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbiMjGzgloUw",
        "outputId": "22115ac2-f53b-4d75-a471-81198b1e2661"
      },
      "id": "BbiMjGzgloUw",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\n",
            "\n",
            "System Configuration:\n",
            " -> Hardware:    NVIDIA A100-SXM4-40GB\n",
            " -> Batch Size:  512 (Optimized)\n",
            " -> Workers:     8\n",
            " -> Mode:        REAL TRAINING\n",
            "\n",
            "========================================\n",
            "RUN: M_base_25\n",
            "========================================\n",
            "\n",
            "[Data] Subset 25.0%: 75000 images, 1500 classes.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 244MB/s]\n",
            "/tmp/ipython-input-3002377437.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESUME] Found: M_base_25_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_25...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_50\n",
            "========================================\n",
            "\n",
            "[Data] Subset 50.0%: 150000 images, 3000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_50_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_50...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_100\n",
            "========================================\n",
            "\n",
            "[Data] Subset 100.0%: 300000 images, 6000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_100_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_100...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "PHASE 4 COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Pfad anpassen falls nötig\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "MODELS_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning', 'models', 'base_models')\n",
        "\n",
        "print(f\"Lese Ergebnisse aus: {MODELS_DIR}\\n\")\n",
        "\n",
        "subsets = [25, 50, 100]\n",
        "\n",
        "for s in subsets:\n",
        "    # Wir suchen nach der _checkpoint Datei, da diese die Metadaten hat\n",
        "    filename = f\"M_base_{s}_checkpoint.pth\"\n",
        "    path = os.path.join(MODELS_DIR, filename)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            # Wir laden auf CPU, das geht schneller\n",
        "            checkpoint = torch.load(path, map_location='cpu')\n",
        "\n",
        "            acc = checkpoint.get('best_acc', -1)\n",
        "            epoch = checkpoint.get('epoch', -1)\n",
        "\n",
        "            print(f\"Modell {s}%:\")\n",
        "            print(f\"  -> Best Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "            print(f\"  -> Gestoppt nach Epoche: {epoch+1}\")\n",
        "            print(\"-\" * 30)\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Lesen von {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"WARNUNG: Checkpoint {filename} nicht gefunden. Nur _best.pth vorhanden?\")"
      ],
      "metadata": {
        "id": "XtyyskInML8W",
        "outputId": "907b068c-256a-495c-f8c6-adb0e7dc87c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XtyyskInML8W",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lese Ergebnisse aus: /content/drive/MyDrive/Deep Learning/models/base_models\n",
            "\n",
            "Modell 25%:\n",
            "  -> Best Accuracy: 0.4552 (45.52%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 50%:\n",
            "  -> Best Accuracy: 0.3140 (31.40%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 100%:\n",
            "  -> Best Accuracy: 0.3681 (36.81%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 5: ROBUST META-LEARNING (MAML) WITH CHECKPOINTING\n",
        "#################################################################\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"\\n--- PHASE 5: Pipeline 1 - Meta-Learning (MAML) ---\")\n",
        "\n",
        "# --- 0. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "BASE_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "MAML_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'maml_models')\n",
        "CHECKPOINT_DIR_LOC = '/content/checkpoints'\n",
        "\n",
        "os.makedirs(MAML_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR_LOC, exist_ok=True)\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MAML_CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "\n",
        "    # --- CONTROL CENTER ---\n",
        "    'DRY_RUN': True,             # <--- CHANGE TO FALSE FOR REAL RUN\n",
        "    'META_ITERATIONS': 2000,     # Total updates (1k-5k recommended)\n",
        "    'VAL_INTERVAL': 100,         # Validate every X steps\n",
        "    # ----------------------\n",
        "\n",
        "    'META_BATCH_SIZE': 4,        # Tasks per update\n",
        "    'WAYS': 5,\n",
        "    'SHOTS': 5,\n",
        "    'QUERY_SHOTS': 15,\n",
        "\n",
        "    'INNER_LR': 0.01,            # Fast Adaptation LR\n",
        "    'INNER_STEPS': 5,            # Updates in Inner Loop\n",
        "    'META_LR': 1e-4,             # Outer Loop LR\n",
        "\n",
        "    'SUBSETS': [0.25, 0.50, 1.0]\n",
        "}\n",
        "\n",
        "print(f\"MAML Config: {MAML_CONFIG['META_ITERATIONS']} Iters, Batch {MAML_CONFIG['META_BATCH_SIZE']}\")\n",
        "print(f\"Mode: {'DRY RUN' if MAML_CONFIG['DRY_RUN'] else 'REAL TRAINING'}\")\n",
        "\n",
        "\n",
        "# --- 2. HELPERS: CHECKPOINTING ---\n",
        "def safe_copy_to_drive(local_path, filename, max_retries=5):\n",
        "    drive_path = os.path.join(MAML_MODELS_DIR, filename)\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            shutil.copy(local_path, drive_path)\n",
        "            if os.path.exists(drive_path) and os.path.getsize(drive_path) > 0:\n",
        "                # Only verbose on first save or retries to keep log clean\n",
        "                return\n",
        "        except Exception:\n",
        "            time.sleep(2 * attempt)\n",
        "    print(f\"   [CRITICAL] Failed to copy {filename} to Drive.\")\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model_state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(model_state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "    print(f\"   [New Best] Saved {filename}\")\n",
        "\n",
        "\n",
        "# --- 3. HELPERS: MODEL LOADING ---\n",
        "def load_base_model_for_maml(fraction, arch='resnet34'):\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    # Try to find the BEST base model first\n",
        "    candidates = [\n",
        "        f\"{subset_name}_best.pth\",\n",
        "        f\"{subset_name}_dryrun_best.pth\",\n",
        "        f\"{subset_name}_checkpoint.pth\"\n",
        "    ]\n",
        "\n",
        "    path = None\n",
        "    for c in candidates:\n",
        "        p = os.path.join(BASE_MODELS_DIR, c)\n",
        "        if os.path.exists(p):\n",
        "            path = p\n",
        "            break\n",
        "\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(f\"No base model found for {subset_name}\")\n",
        "\n",
        "    print(f\"   -> Initializing from: {os.path.basename(path)}\")\n",
        "\n",
        "    full_classes = 6000\n",
        "    num_classes = int(full_classes * fraction)\n",
        "\n",
        "    if arch == 'resnet34':\n",
        "        model = models.resnet34(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    state_dict = torch.load(path, map_location='cpu')\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 4. HELPER: VALIDATION (Simulate Adaptation) ---\n",
        "def evaluate_on_val_set(meta_model, val_generator, criterion, device):\n",
        "    \"\"\"\n",
        "    Simulates adaptation on unseen tasks (C_val) and measures Accuracy.\n",
        "    Crucial for picking the 'Best' MAML model.\n",
        "    \"\"\"\n",
        "    meta_model.eval()\n",
        "    # Dry run: check 2 tasks. Real run: check 20 tasks for stable stats.\n",
        "    num_tasks = 2 if MAML_CONFIG['DRY_RUN'] else 20\n",
        "\n",
        "    total_acc = 0.0\n",
        "\n",
        "    # Loop over validation tasks\n",
        "    for _ in range(num_tasks):\n",
        "        batch_data, batch_labels = val_generator.sample()\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "\n",
        "        # Split Support/Query (Same logic as training)\n",
        "        ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "        support_indices, query_indices = [], []\n",
        "        for w in range(ways):\n",
        "            base = w * (shots + queries)\n",
        "            support_indices.extend(range(base, base + shots))\n",
        "            query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "        supp_X, supp_y = batch_data[support_indices], batch_labels[support_indices]\n",
        "        query_X, query_y = batch_data[query_indices], batch_labels[query_indices]\n",
        "\n",
        "        # --- INNER LOOP (Adaptation) ---\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train() # BN needs to track stats even in val adaptation\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "        for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        # --- EVALUATION (Query) ---\n",
        "        fast_model.eval()\n",
        "        with torch.no_grad():\n",
        "            q_preds = fast_model(query_X)\n",
        "            _, predicted = torch.max(q_preds.data, 1)\n",
        "            correct = (predicted == query_y).sum().item()\n",
        "            acc = correct / query_y.size(0)\n",
        "            total_acc += acc\n",
        "\n",
        "        del fast_model, inner_opt\n",
        "\n",
        "    avg_acc = total_acc / num_tasks\n",
        "    meta_model.train() # Switch back to train mode\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "# --- 5. CORE: MANUAL MAML STEP ---\n",
        "def manual_fomaml_step(meta_model, task_generator, meta_optimizer, criterion, device):\n",
        "    meta_loss_total = 0.0\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    for _ in range(MAML_CONFIG['META_BATCH_SIZE']):\n",
        "        # 1. Data\n",
        "        batch_data, batch_labels = task_generator.sample()\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "\n",
        "        ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "        support_indices, query_indices = [], []\n",
        "        for w in range(ways):\n",
        "            base = w * (shots + queries)\n",
        "            support_indices.extend(range(base, base + shots))\n",
        "            query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "        supp_X, supp_y = batch_data[support_indices], batch_labels[support_indices]\n",
        "        query_X, query_y = batch_data[query_indices], batch_labels[query_indices]\n",
        "\n",
        "        # 2. Clone & Adapt\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train()\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "        for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        # 3. Query Loss & Gradient Accumulation (First Order)\n",
        "        q_preds = fast_model(query_X)\n",
        "        q_loss = criterion(q_preds, query_y)\n",
        "        meta_loss_total += q_loss.item()\n",
        "\n",
        "        q_loss.backward()\n",
        "\n",
        "        # Transfer gradients to meta_model\n",
        "        for mp, fp in zip(meta_model.parameters(), fast_model.parameters()):\n",
        "            if fp.grad is not None:\n",
        "                grad = fp.grad.detach() / MAML_CONFIG['META_BATCH_SIZE']\n",
        "                if mp.grad is None: mp.grad = grad\n",
        "                else: mp.grad += grad\n",
        "\n",
        "        del fast_model, inner_opt, q_loss\n",
        "\n",
        "    meta_optimizer.step()\n",
        "    return meta_loss_total / MAML_CONFIG['META_BATCH_SIZE']\n",
        "\n",
        "\n",
        "# --- 6. MAIN LOOP ---\n",
        "def run_maml_training(fraction):\n",
        "    maml_name = f\"M_maml_{int(fraction*100)}\"\n",
        "    run_tag = \"_dryrun\" if MAML_CONFIG['DRY_RUN'] else \"\"\n",
        "\n",
        "    # Filenames\n",
        "    ckpt_file = f\"{maml_name}{run_tag}_checkpoint.pth\"\n",
        "    best_file = f\"{maml_name}{run_tag}_best.pth\"\n",
        "\n",
        "    print(f\"\\n{'='*40}\\nMETA-TRAINING: {maml_name}{run_tag}\\n{'='*40}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load Data\n",
        "    train_gen = get_episodic_taskset(split='c_base', ways=5, shots=5, query_shots=15)\n",
        "    val_gen = get_episodic_taskset(split='c_val', ways=5, shots=5, query_shots=15)\n",
        "\n",
        "    # Load Model\n",
        "    meta_model = load_base_model_for_maml(fraction, MAML_CONFIG['ARCH'])\n",
        "    meta_model = meta_model.to(device)\n",
        "\n",
        "    meta_optimizer = optim.Adam(meta_model.parameters(), lr=MAML_CONFIG['META_LR'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Resume Logic\n",
        "    start_iter = 0\n",
        "    best_val_acc = -1.0\n",
        "\n",
        "    drive_ckpt_path = os.path.join(MAML_MODELS_DIR, ckpt_file)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"[RESUME] Found {ckpt_file}\")\n",
        "        try:\n",
        "            ckpt = torch.load(drive_ckpt_path, map_location=device)\n",
        "            if MAML_CONFIG['DRY_RUN']:\n",
        "                 print(\"   -> (Dry Run) Resetting loop to 0.\")\n",
        "                 start_iter = 0\n",
        "                 best_val_acc = ckpt.get('best_val_acc', -1.0)\n",
        "            else:\n",
        "                 start_iter = ckpt['iteration'] + 1\n",
        "                 best_val_acc = ckpt.get('best_val_acc', 0.0)\n",
        "                 meta_model.load_state_dict(ckpt['model_state_dict'])\n",
        "                 meta_optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "                 print(f\"   -> Resuming from iter {start_iter} (Best Val Acc: {best_val_acc:.4f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"   [Error] Corrupt checkpoint ({e}). Restarting.\")\n",
        "\n",
        "    # Loop\n",
        "    total_iters = 5 if MAML_CONFIG['DRY_RUN'] else MAML_CONFIG['META_ITERATIONS']\n",
        "    pbar = tqdm(range(start_iter, total_iters), desc=f\"{maml_name}\")\n",
        "\n",
        "    for i in pbar:\n",
        "        # 1. Update Step\n",
        "        loss = manual_fomaml_step(meta_model, train_gen, meta_optimizer, criterion, device)\n",
        "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
        "\n",
        "        # 2. Validation & Saving\n",
        "        if i % MAML_CONFIG['VAL_INTERVAL'] == 0 or i == total_iters - 1:\n",
        "            val_acc = evaluate_on_val_set(meta_model, val_gen, criterion, device)\n",
        "\n",
        "            # Feedback\n",
        "            print(f\"   Iter {i}: Meta Loss {loss:.4f} | Val Acc {val_acc:.4f} (Best: {best_val_acc:.4f})\")\n",
        "\n",
        "            # Save Full Checkpoint (Resume)\n",
        "            state = {\n",
        "                'iteration': i,\n",
        "                'model_state_dict': meta_model.state_dict(),\n",
        "                'optimizer_state_dict': meta_optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc\n",
        "            }\n",
        "            save_checkpoint(state, ckpt_file)\n",
        "\n",
        "            # Save Best Model (Weights Only)\n",
        "            save_condition = False\n",
        "            if val_acc > best_val_acc: save_condition = True\n",
        "            elif best_val_acc == -1.0: save_condition = True # First run logic\n",
        "            elif MAML_CONFIG['DRY_RUN'] and val_acc >= best_val_acc: save_condition = True\n",
        "\n",
        "            if save_condition:\n",
        "                best_val_acc = val_acc\n",
        "                save_best_model(meta_model.state_dict(), best_file)\n",
        "\n",
        "    print(f\"Training Complete. Final Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del meta_model, meta_optimizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 7. EXECUTION ---\n",
        "# Ensure Data Loaders exist\n",
        "if 'get_episodic_taskset' not in globals():\n",
        "    raise NameError(\"Please run Step 2.3 (Data Loaders) first!\")\n",
        "\n",
        "for fraction in MAML_CONFIG['SUBSETS']:\n",
        "    run_maml_training(fraction)\n",
        "\n",
        "print(\"\\nPHASE 5 COMPLETE.\")"
      ],
      "metadata": {
        "id": "PLNuhzHGHswM",
        "outputId": "812c4af7-c4bd-42b2-f439-8baa1c938003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f22c305e36b5451ca4cafb3620b6e68d",
            "c78e0b8d45c64c2aac178b646500cb79",
            "02c138e6e1554831a58d7d3ae0cb1332",
            "d43ad66f16f0448288fa276dec45ff50",
            "38dd20c93fd9449997615fbf9b6c0055",
            "81c7726104f2447fa86225bfe6aba531",
            "1d1987835c8e4793a4e7ed0154e35ed6",
            "682e2152c5f24e3a87d23ea17f853707",
            "3f23fbd2593047e8b9878c036b7d9ecf",
            "feaeb0cc8d5342478919dd5e42259d1d",
            "19e564b9fa3e4634b2eae0d077f8af20",
            "b6c5aa430c8c4a95b09b4e8bb602e43d",
            "4ed125dc64044318a539fa75976f1db5",
            "5c01351e8eeb4b22b445de5bbad08ac3",
            "7117777f530d4379b0184f15747969f5",
            "4e06d85d988e4e19b50b9ae0063fda62",
            "0d0e1af5727f4c0fbf9229f8e583504d",
            "686ce3a7ceeb48d58987fe7eaa75a385",
            "c1bacd0e6e1b417d99209b470d7cdbdb",
            "30d8121d851e4496ab37650c0da6acb9",
            "b48da9bfc45d40f2952f23fa0e0664da",
            "a2e65d87eb8f48ed84770abe44d8a687",
            "3ed5f74012654d0b9ea7563e26860501",
            "34a9c23f57d44ede9a9801de923e6cce",
            "20c347253aae4b5fb92abf7059966cdc",
            "85cedf106eae4c3695a438e34690144d",
            "4a122f64d1684e40ab91f2e9b8a9d14e",
            "6f34991c91024242b6e91c7bbd4fa818",
            "9393b91059d74178b0f014d1cdc1a40c",
            "40d8b6771e254d87a9900e277cf6bd9a",
            "28eaeeba928a4243b86c1b24e63e0d82",
            "60fe6a3b1da44e04a3205c5b1bc0795d",
            "7cfdcdf44ad24fd3a920bbc29426f532"
          ]
        }
      },
      "id": "PLNuhzHGHswM",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHASE 5: Pipeline 1 - Meta-Learning (MAML) ---\n",
            "MAML Config: 2000 Iters, Batch 4\n",
            "Mode: DRY RUN\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_25_dryrun\n",
            "========================================\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_val'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "   -> Initializing from: M_base_25_best.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "M_maml_25:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22c305e36b5451ca4cafb3620b6e68d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Iter 0: Meta Loss 13.1774 | Val Acc 0.0267 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_25_dryrun_best.pth\n",
            "   Iter 4: Meta Loss 12.5350 | Val Acc 0.1133 (Best: 0.0267)\n",
            "   [New Best] Saved M_maml_25_dryrun_best.pth\n",
            "Training Complete. Final Best Val Acc: 0.1133\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_50_dryrun\n",
            "========================================\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_val'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "   -> Initializing from: M_base_50_best.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "M_maml_50:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c5aa430c8c4a95b09b4e8bb602e43d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Iter 0: Meta Loss 15.8054 | Val Acc 0.0467 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_50_dryrun_best.pth\n",
            "   Iter 4: Meta Loss 15.8234 | Val Acc 0.0400 (Best: 0.0467)\n",
            "Training Complete. Final Best Val Acc: 0.0467\n",
            "\n",
            "========================================\n",
            "META-TRAINING: M_maml_100_dryrun\n",
            "========================================\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_val'...\n",
            " -> Configured 5-Way 5-Shot Tasks (Native PyTorch).\n",
            "   -> Initializing from: M_base_100_best.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "M_maml_100:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ed5f74012654d0b9ea7563e26860501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Iter 0: Meta Loss 18.9555 | Val Acc 0.0200 (Best: -1.0000)\n",
            "   [New Best] Saved M_maml_100_dryrun_best.pth\n",
            "   Iter 4: Meta Loss 17.8747 | Val Acc 0.0000 (Best: 0.0200)\n",
            "Training Complete. Final Best Val Acc: 0.0200\n",
            "\n",
            "PHASE 5 COMPLETE.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f22c305e36b5451ca4cafb3620b6e68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c78e0b8d45c64c2aac178b646500cb79",
              "IPY_MODEL_02c138e6e1554831a58d7d3ae0cb1332",
              "IPY_MODEL_d43ad66f16f0448288fa276dec45ff50"
            ],
            "layout": "IPY_MODEL_38dd20c93fd9449997615fbf9b6c0055"
          }
        },
        "c78e0b8d45c64c2aac178b646500cb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81c7726104f2447fa86225bfe6aba531",
            "placeholder": "​",
            "style": "IPY_MODEL_1d1987835c8e4793a4e7ed0154e35ed6",
            "value": "M_maml_25: 100%"
          }
        },
        "02c138e6e1554831a58d7d3ae0cb1332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_682e2152c5f24e3a87d23ea17f853707",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f23fbd2593047e8b9878c036b7d9ecf",
            "value": 5
          }
        },
        "d43ad66f16f0448288fa276dec45ff50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feaeb0cc8d5342478919dd5e42259d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_19e564b9fa3e4634b2eae0d077f8af20",
            "value": " 5/5 [00:22&lt;00:00,  5.14s/it, loss=12.5350]"
          }
        },
        "38dd20c93fd9449997615fbf9b6c0055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81c7726104f2447fa86225bfe6aba531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1987835c8e4793a4e7ed0154e35ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "682e2152c5f24e3a87d23ea17f853707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f23fbd2593047e8b9878c036b7d9ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "feaeb0cc8d5342478919dd5e42259d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e564b9fa3e4634b2eae0d077f8af20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c5aa430c8c4a95b09b4e8bb602e43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ed125dc64044318a539fa75976f1db5",
              "IPY_MODEL_5c01351e8eeb4b22b445de5bbad08ac3",
              "IPY_MODEL_7117777f530d4379b0184f15747969f5"
            ],
            "layout": "IPY_MODEL_4e06d85d988e4e19b50b9ae0063fda62"
          }
        },
        "4ed125dc64044318a539fa75976f1db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0e1af5727f4c0fbf9229f8e583504d",
            "placeholder": "​",
            "style": "IPY_MODEL_686ce3a7ceeb48d58987fe7eaa75a385",
            "value": "M_maml_50: 100%"
          }
        },
        "5c01351e8eeb4b22b445de5bbad08ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1bacd0e6e1b417d99209b470d7cdbdb",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30d8121d851e4496ab37650c0da6acb9",
            "value": 5
          }
        },
        "7117777f530d4379b0184f15747969f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b48da9bfc45d40f2952f23fa0e0664da",
            "placeholder": "​",
            "style": "IPY_MODEL_a2e65d87eb8f48ed84770abe44d8a687",
            "value": " 5/5 [00:17&lt;00:00,  3.58s/it, loss=15.8234]"
          }
        },
        "4e06d85d988e4e19b50b9ae0063fda62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0e1af5727f4c0fbf9229f8e583504d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686ce3a7ceeb48d58987fe7eaa75a385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1bacd0e6e1b417d99209b470d7cdbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30d8121d851e4496ab37650c0da6acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b48da9bfc45d40f2952f23fa0e0664da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e65d87eb8f48ed84770abe44d8a687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ed5f74012654d0b9ea7563e26860501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34a9c23f57d44ede9a9801de923e6cce",
              "IPY_MODEL_20c347253aae4b5fb92abf7059966cdc",
              "IPY_MODEL_85cedf106eae4c3695a438e34690144d"
            ],
            "layout": "IPY_MODEL_4a122f64d1684e40ab91f2e9b8a9d14e"
          }
        },
        "34a9c23f57d44ede9a9801de923e6cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f34991c91024242b6e91c7bbd4fa818",
            "placeholder": "​",
            "style": "IPY_MODEL_9393b91059d74178b0f014d1cdc1a40c",
            "value": "M_maml_100: 100%"
          }
        },
        "20c347253aae4b5fb92abf7059966cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d8b6771e254d87a9900e277cf6bd9a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28eaeeba928a4243b86c1b24e63e0d82",
            "value": 5
          }
        },
        "85cedf106eae4c3695a438e34690144d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60fe6a3b1da44e04a3205c5b1bc0795d",
            "placeholder": "​",
            "style": "IPY_MODEL_7cfdcdf44ad24fd3a920bbc29426f532",
            "value": " 5/5 [00:19&lt;00:00,  4.37s/it, loss=17.8747]"
          }
        },
        "4a122f64d1684e40ab91f2e9b8a9d14e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f34991c91024242b6e91c7bbd4fa818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9393b91059d74178b0f014d1cdc1a40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40d8b6771e254d87a9900e277cf6bd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28eaeeba928a4243b86c1b24e63e0d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60fe6a3b1da44e04a3205c5b1bc0795d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cfdcdf44ad24fd3a920bbc29426f532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}