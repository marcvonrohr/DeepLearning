{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcvonrohr/DeepLearning/blob/main/meta_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54242527",
      "metadata": {
        "id": "54242527",
        "outputId": "bb1ec5bc-b721-4f7a-b5b7-7a6112d010d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "...Google Drive connected.\n",
            "Local data directory created at: /content/data/inaturalist_unpacked\n",
            "\n",
            "--- Processing 2021_train_mini ---\n",
            "Copying '2021_train_mini.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 1056.15 seconds.\n",
            "Unpacking '2021_train_mini.tar.gz' locally...\n",
            "...Unpacking complete. Took 441.59 seconds.\n",
            "Deleting local tarball '/content/data/2021_train_mini.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Processing 2021_valid ---\n",
            "Copying '2021_valid.tar.gz' from Drive to local VM...\n",
            "...Copy complete. Took 213.07 seconds.\n",
            "Unpacking '2021_valid.tar.gz' locally...\n",
            "...Unpacking complete. Took 77.47 seconds.\n",
            "Deleting local tarball '/content/data/2021_valid.tar.gz'...\n",
            "...Local tarball deleted.\n",
            "\n",
            "--- Final Data Setup Verification ---\n",
            "Dataset is ready for training at: /content/data/inaturalist_unpacked\n",
            "total 2.5M\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 train_mini\n",
            "drwxrwxr-x 10002 1000 1000 1.3M Oct 13  2020 val\n",
            "\n",
            "Local VM Disk Space Usage:\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   98G  139G  42% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              41G     0   41G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "/dev/sda1       242G  150G   93G  62% /opt/bin/.nvidia\n",
            "tmpfs            42G  1.8M   42G   1% /var/colab\n",
            "tmpfs            42G     0   42G   0% /proc/acpi\n",
            "tmpfs            42G     0   42G   0% /proc/scsi\n",
            "tmpfs            42G     0   42G   0% /sys/firmware\n",
            "drive           236G  105G  132G  45% /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "#################################################################\n",
        "#  STEP 2.1: PREPARE LOCAL VM\n",
        "#################################################################\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "print(\"Connecting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"...Google Drive connected.\")\n",
        "\n",
        "# --- 2. Define Key Paths ---\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "DATASETS_ROOT_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
        "INAT_ROOT_DIR = os.path.join(DATASETS_ROOT_DIR, 'inaturalist')\n",
        "\n",
        "# Source: The COMPRESSED archives\n",
        "ARCHIVES_DIR_ON_DRIVE = os.path.join(INAT_ROOT_DIR, 'archives')\n",
        "\n",
        "# Target: The LOCAL VM fast disk\n",
        "LOCAL_DATA_ROOT = '/content/data'\n",
        "# This is the final path your PyTorch code will use:\n",
        "FINAL_DATA_PATH = os.path.join(LOCAL_DATA_ROOT, 'inaturalist_unpacked')\n",
        "\n",
        "# Define source/destination paths\n",
        "TAR_FILES = {\n",
        "    \"2021_train_mini\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_train_mini.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_train_mini.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_train_mini')\n",
        "    },\n",
        "    \"2021_valid\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_valid.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_valid.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_valid')\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 3. Create Local Directories on VM ---\n",
        "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(FINAL_DATA_PATH, exist_ok=True)\n",
        "print(f\"Local data directory created at: {FINAL_DATA_PATH}\")\n",
        "\n",
        "# --- 4. Copy, Unpack, and Clean up for each file ---\n",
        "for name, paths in TAR_FILES.items():\n",
        "    print(f\"\\n--- Processing {name} ---\")\n",
        "\n",
        "    if os.path.exists(paths[\"check_unpacked\"]):\n",
        "        print(f\"'{name}' is already unpacked in local VM. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 4a. Copy .tar.gz from Drive to local VM\n",
        "    print(f\"Copying '{name}.tar.gz' from Drive to local VM...\")\n",
        "    start_time = time.time()\n",
        "    !cp \"{paths['src']}\" \"{paths['dest_tar']}\"\n",
        "    print(f\"...Copy complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4b. Unpack the file on the local VM\n",
        "    print(f\"Unpacking '{name}.tar.gz' locally...\")\n",
        "    start_time = time.time()\n",
        "    !tar -xzf \"{paths['dest_tar']}\" -C \"{FINAL_DATA_PATH}\"\n",
        "    print(f\"...Unpacking complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4c. Delete the local .tar.gz file to save VM space\n",
        "    print(f\"Deleting local tarball '{paths['dest_tar']}'...\")\n",
        "    !rm \"{paths['dest_tar']}\"\n",
        "    print(\"...Local tarball deleted.\")\n",
        "\n",
        "# --- 5. Verify and Set Path for Training ---\n",
        "print(\"\\n--- Final Data Setup Verification ---\")\n",
        "print(f\"Dataset is ready for training at: {FINAL_DATA_PATH}\")\n",
        "!ls -lh \"{FINAL_DATA_PATH}\"\n",
        "print(\"\\nLocal VM Disk Space Usage:\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.2: SCIENTIFIC DATA PARTITIONING\n",
        "#################################################################\n",
        "print(\"\\n--- STEP 2.2: Loading/Creating Scientific Class Partition ---\")\n",
        "\n",
        "# --- 6. Define Paths for Partition File ---\n",
        "# We create a 'project_meta' folder on GDrive to store helper files\n",
        "META_DIR_ON_DRIVE = os.path.join(PROJECT_DIR, 'project_meta')\n",
        "os.makedirs(META_DIR_ON_DRIVE, exist_ok=True)\n",
        "\n",
        "PARTITION_FILE_PATH = os.path.join(META_DIR_ON_DRIVE, 'inat_class_split.json')\n",
        "print(f\"Looking for partition file at: {PARTITION_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_ARRY2GUvXt",
        "outputId": "b4ba3eb7-3994-4dd1-81d0-bdb3e668c922"
      },
      "id": "1_ARRY2GUvXt",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.2: Loading/Creating Scientific Class Partition ---\n",
            "Looking for partition file at: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Logic to Find Classes and Create Partition ---\n",
        "\n",
        "# 7a. Identify the Dataset Root\n",
        "# The unpacking might have created a subfolder (e.g., '2021_train_mini' or 'train_mini')\n",
        "# or files might be directly in FINAL_DATA_PATH. We check common patterns.\n",
        "possible_roots = [\n",
        "    os.path.join(FINAL_DATA_PATH, '2021_train_mini'),\n",
        "    os.path.join(FINAL_DATA_PATH, 'train_mini'),\n",
        "    FINAL_DATA_PATH\n",
        "]\n",
        "\n",
        "DATASET_ROOT = None\n",
        "for path in possible_roots:\n",
        "    if os.path.exists(path):\n",
        "        # Check if this path actually contains subdirectories\n",
        "        if len([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]) > 0:\n",
        "            DATASET_ROOT = path\n",
        "            break\n",
        "\n",
        "print(f\"Dataset root identified as: {DATASET_ROOT}\")\n",
        "\n",
        "# 7b. Load or Create the Partition\n",
        "partition_data = {}\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "if os.path.exists(PARTITION_FILE_PATH):\n",
        "    print(\"Found existing partition file. Loading...\")\n",
        "    with open(PARTITION_FILE_PATH, 'r') as f:\n",
        "        partition_data = json.load(f)\n",
        "else:\n",
        "    print(\"No partition file found. Scanning directories to create new partition...\")\n",
        "    print(\"This ensures independence from missing metadata files.\")\n",
        "\n",
        "    # --- Scan for Class Folders ---\n",
        "    class_folders_rel = []\n",
        "\n",
        "    # Walk through the directory tree\n",
        "    # A \"class\" is any folder that contains image files (.jpg, .jpeg, .png)\n",
        "    print(\"Scanning folders (this may take 1-2 minutes)...\")\n",
        "    for root, dirs, files in os.walk(DATASET_ROOT):\n",
        "        # Check for images in this specific folder\n",
        "        images = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if len(images) > 0:\n",
        "            # Get path relative to the dataset root (e.g., \"Aves/Turdus_migratorius\")\n",
        "            rel_path = os.path.relpath(root, DATASET_ROOT)\n",
        "            class_folders_rel.append(rel_path)\n",
        "\n",
        "    # --- CRITICAL: Sort for Reproducibility ---\n",
        "    # Sorting ensures that Index 0 is ALWAYS the same class on every machine/run\n",
        "    class_folders_rel.sort()\n",
        "\n",
        "    num_classes = len(class_folders_rel)\n",
        "    print(f\"Found {num_classes} classes containing images.\")\n",
        "\n",
        "    if num_classes < 9900:\n",
        "        print(\"WARNING: Found significantly fewer than 10,000 classes. Check extraction.\")\n",
        "\n",
        "    # --- Assign IDs and Shuffle ---\n",
        "    all_class_ids = list(range(num_classes))\n",
        "\n",
        "    print(f\"Shuffling {num_classes} class IDs with random seed {RANDOM_SEED}...\")\n",
        "    random.seed(RANDOM_SEED)\n",
        "    random.shuffle(all_class_ids)\n",
        "\n",
        "    # --- Split into Sets ---\n",
        "    # 6000 Base (Train/Meta-Train), 2000 Val (Hyperparams), 2000 Novel (Test)\n",
        "    c_base_ids = all_class_ids[:6000]\n",
        "    c_val_ids = all_class_ids[6000:8000]\n",
        "    c_novel_ids = all_class_ids[8000:]\n",
        "\n",
        "    # --- Construct Data Structure ---\n",
        "    # We save both the sets AND the mapping from ID -> Folder Path\n",
        "    partition_data = {\n",
        "        \"sets\": {\n",
        "            'c_base': sorted(c_base_ids),\n",
        "            'c_val': sorted(c_val_ids),\n",
        "            'c_novel': sorted(c_novel_ids)\n",
        "        },\n",
        "        \"id_to_path\": {\n",
        "            str(i): folder_path for i, folder_path in enumerate(class_folders_rel)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    print(f\"Saving new partition and mapping to: {PARTITION_FILE_PATH}\")\n",
        "    with open(PARTITION_FILE_PATH, 'w') as f:\n",
        "        json.dump(partition_data, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987lbsNFU4Qf",
        "outputId": "caaa5891-8a54-4609-db90-ff2487150b8b"
      },
      "id": "987lbsNFU4Qf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset root identified as: /content/data/inaturalist_unpacked/train_mini\n",
            "Found existing partition file. Loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Verification ---\n",
        "print(\"\\n--- Partitioning Complete ---\")\n",
        "sets = partition_data['sets']\n",
        "print(f\"Total C_base classes:  {len(sets['c_base'])}\")\n",
        "print(f\"Total C_val classes:   {len(sets['c_val'])}\")\n",
        "print(f\"Total C_novel classes: {len(sets['c_novel'])}\")\n",
        "\n",
        "# Check for overlaps (should be 0)\n",
        "base_set = set(sets['c_base'])\n",
        "val_set = set(sets['c_val'])\n",
        "novel_set = set(sets['c_novel'])\n",
        "\n",
        "overlap_bv = base_set & val_set\n",
        "overlap_bn = base_set & novel_set\n",
        "overlap_vn = val_set & novel_set\n",
        "\n",
        "print(f\"Overlap (Base-Val):    {len(overlap_bv)}\")\n",
        "print(f\"Overlap (Base-Novel):  {len(overlap_bn)}\")\n",
        "print(f\"Overlap (Val-Novel):   {len(overlap_vn)}\")\n",
        "\n",
        "if len(overlap_bv) + len(overlap_bn) + len(overlap_vn) == 0:\n",
        "    print(\"\\nSUCCESS: Classes are cleanly partitioned.\")\n",
        "else:\n",
        "    print(\"\\nCRITICAL ERROR: Overlaps detected in class sets!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8xIL6YPU6OM",
        "outputId": "844ca9fb-d659-41ad-d745-3f5f5ed2f681"
      },
      "id": "w8xIL6YPU6OM",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Partitioning Complete ---\n",
            "Total C_base classes:  6000\n",
            "Total C_val classes:   2000\n",
            "Total C_novel classes: 2000\n",
            "Overlap (Base-Val):    0\n",
            "Overlap (Base-Novel):  0\n",
            "Overlap (Val-Novel):   0\n",
            "\n",
            "SUCCESS: Classes are cleanly partitioned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.3: MODULAR DATA LOADERS (NO LEARN2LEARN DEPENDENCY)\n",
        "#################################################################\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "print(\"\\n--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\")\n",
        "\n",
        "# --- SAFETY CHECK ---\n",
        "# Ensure variables from Step 2.2 exist\n",
        "required_vars = ['DATASET_ROOT', 'PARTITION_FILE_PATH']\n",
        "if not all(v in globals() for v in required_vars):\n",
        "    raise NameError(f\"Missing variables from Step 2.2. Please run the previous cell.\")\n",
        "\n",
        "print(f\"Using Dataset Root: {DATASET_ROOT}\")\n",
        "print(f\"Using Partition File: {PARTITION_FILE_PATH}\")\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZE_STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ7TNDctB_kt",
        "outputId": "2bb5fe45-a449-43f9-a76d-0949e71164b0"
      },
      "id": "ZZ7TNDctB_kt",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2.3: Initialize Custom Data Loaders (Native PyTorch) ---\n",
            "Using Dataset Root: /content/data/inaturalist_unpacked/train_mini\n",
            "Using Partition File: /content/drive/MyDrive/Deep Learning/project_meta/inat_class_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  CORE COMPONENT: The Custom Dataset Class\n",
        "# ==============================================================================\n",
        "class MetaINatDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset that enforces the scientific partition.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, partition_file, split='c_base', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        with open(partition_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if split not in data['sets']:\n",
        "            raise ValueError(f\"Invalid split '{split}'. Available: {list(data['sets'].keys())}\")\n",
        "\n",
        "        self.allowed_ids = data['sets'][split]\n",
        "        self.id_to_path = data['id_to_path']\n",
        "\n",
        "        # Map original ID -> 0..N-1\n",
        "        self.label_map = {orig: new for new, orig in enumerate(self.allowed_ids)}\n",
        "\n",
        "        self.samples = []\n",
        "        for original_id in self.allowed_ids:\n",
        "            rel_path = self.id_to_path[str(original_id)]\n",
        "            abs_path = os.path.join(self.root_dir, rel_path)\n",
        "            if os.path.exists(abs_path):\n",
        "                for img in os.listdir(abs_path):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        self.samples.append({\n",
        "                            'path': os.path.join(abs_path, img),\n",
        "                            'label': self.label_map[original_id]\n",
        "                        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = Image.open(sample['path']).convert('RGB')\n",
        "        label = sample['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ykdP8nDFcU6i"
      },
      "id": "ykdP8nDFcU6i",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  HELPER: Episodic Batch Generator (Replaces learn2learn)\n",
        "# ==============================================================================\n",
        "class EpisodicTaskGenerator:\n",
        "    \"\"\"\n",
        "    Native PyTorch implementation of an N-Way K-Shot task sampler.\n",
        "    Replaces learn2learn functionality without installation issues.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, ways, shots, query_shots):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Group all image indices by their label for fast sampling\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl not in self.indices_by_label:\n",
        "                self.indices_by_label[lbl] = []\n",
        "            self.indices_by_label[lbl].append(idx)\n",
        "\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # 1. Sample N random classes (Ways)\n",
        "        selected_classes = random.sample(self.classes, self.ways)\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        # 2. Sample K + Q images from each class\n",
        "        for local_label, global_label_idx in enumerate(selected_classes):\n",
        "            indices = self.indices_by_label[global_label_idx]\n",
        "\n",
        "            # Ensure we have enough images, otherwise sample with replacement\n",
        "            needed = self.shots + self.query_shots\n",
        "            if len(indices) >= needed:\n",
        "                selected_indices = random.sample(indices, needed)\n",
        "            else:\n",
        "                selected_indices = random.choices(indices, k=needed)\n",
        "\n",
        "            # 3. Load images and re-label them to 0..N-1 for the episode\n",
        "            for idx in selected_indices:\n",
        "                img, _ = self.dataset[idx] # dataset returns (img, global_label)\n",
        "                batch_images.append(img)\n",
        "                # Important: The label for the loss function must be 0..Ways-1\n",
        "                batch_labels.append(local_label)\n",
        "\n",
        "        # Stack into a single tensor: [Ways * (Shots+Query), C, H, W]\n",
        "        data = torch.stack(batch_images)\n",
        "        labels = torch.tensor(batch_labels)\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def sample(self):\n",
        "        # Compatibility method to look like learn2learn\n",
        "        return self.__next__()"
      ],
      "metadata": {
        "id": "asZAUP18cfqQ"
      },
      "id": "asZAUP18cfqQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER A: Standard Pre-Training Loader\n",
        "# ==============================================================================\n",
        "def get_standard_loader(split='c_base', batch_size=64, shuffle=True):\n",
        "    print(f\"\\n[Loader A] Initializing Standard Loader for split '{split}'...\")\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=train_transforms)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\" -> {len(dataset)} total images.\")\n",
        "    print(f\" -> {len(dataset.allowed_ids)} classes.\")\n",
        "    return loader, len(dataset.allowed_ids)"
      ],
      "metadata": {
        "id": "oVtAtf9zcm9e"
      },
      "id": "oVtAtf9zcm9e",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER B: Episodic Task Loader (MAML) - NATIVE IMPLEMENTATION\n",
        "# ==============================================================================\n",
        "def get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1, img_size=84):\n",
        "    print(f\"\\n[Loader B] Initializing Episodic Generator for split '{split}'...\")\n",
        "\n",
        "    maml_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=maml_transforms)\n",
        "\n",
        "    # Use our native generator instead of learn2learn\n",
        "    task_generator = EpisodicTaskGenerator(\n",
        "        dataset,\n",
        "        ways=ways,\n",
        "        shots=shots,\n",
        "        query_shots=query_shots\n",
        "    )\n",
        "\n",
        "    print(f\" -> Configured {ways}-Way {shots}-Shot Tasks (Native PyTorch).\")\n",
        "    return task_generator"
      ],
      "metadata": {
        "id": "madMlUDOc5CK"
      },
      "id": "madMlUDOc5CK",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOADER C: Fixed Few-Shot Loader for FT/LoRA\n",
        "# ==============================================================================\n",
        "def get_fixed_few_shot_task(split='c_novel', ways=5, shots=1, query_shots=15, seed=None):\n",
        "    print(f\"\\n[Loader C] Creating Fixed Few-Shot Task from '{split}'...\")\n",
        "\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    eval_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    dataset = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=eval_transforms)\n",
        "\n",
        "    available_labels = list(set(s['label'] for s in dataset.samples))\n",
        "    selected_classes = random.sample(available_labels, ways)\n",
        "\n",
        "    class_indices = {c: [] for c in selected_classes}\n",
        "    for idx, sample in enumerate(dataset.samples):\n",
        "        if sample['label'] in selected_classes:\n",
        "            class_indices[sample['label']].append(idx)\n",
        "\n",
        "    support_indices = []\n",
        "    query_indices = []\n",
        "\n",
        "    for c in selected_classes:\n",
        "        idxs = class_indices[c]\n",
        "        random.shuffle(idxs)\n",
        "        support_indices.extend(idxs[:shots])\n",
        "        query_indices.extend(idxs[shots : shots+query_shots])\n",
        "\n",
        "    support_loader = DataLoader(Subset(dataset, support_indices), batch_size=16, shuffle=True)\n",
        "    query_loader = DataLoader(Subset(dataset, query_indices), batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\" -> Support Set: {len(support_indices)} images, Query Set: {len(query_indices)} images\")\n",
        "    return support_loader, query_loader"
      ],
      "metadata": {
        "id": "G1m0luxjc5gB"
      },
      "id": "G1m0luxjc5gB",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  VERIFICATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Testing Loaders ---\")\n",
        "\n",
        "# Test A\n",
        "try:\n",
        "    l_std, n_cls = get_standard_loader(split='c_base', batch_size=4)\n",
        "    print(\"Loader A (Standard) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader A Failed: {e}\")\n",
        "\n",
        "# Test B (Now using Native Generator)\n",
        "try:\n",
        "    task_gen = get_episodic_taskset(split='c_base', ways=5, shots=1, query_shots=1)\n",
        "    batch_data, batch_labels = task_gen.sample()\n",
        "    # Expected shape: [Way*(Shot+Query), 3, 84, 84] -> [5*(1+1), 3, 84, 84] = [10, 3, 84, 84]\n",
        "    print(f\"Loader B (Episodic) check: OK. Batch shape: {batch_data.shape}\")\n",
        "    if batch_labels.max() >= 5:\n",
        "        print(\"WARNING: Labels not properly remapped to 0..N-1\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader B Failed: {e}\")\n",
        "\n",
        "# Test C\n",
        "try:\n",
        "    sup_dl, q_dl = get_fixed_few_shot_task(split='c_novel', ways=5, shots=5)\n",
        "    print(\"Loader C (Fixed) check: OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Loader C Failed: {e}\")\n",
        "\n",
        "print(\"\\nStep 2.3 Complete (Dependencies Fixed).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zceGZ_SdEeE",
        "outputId": "2821831c-a697-416a-873e-10be4e6ed81e"
      },
      "id": "3zceGZ_SdEeE",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Loaders ---\n",
            "\n",
            "[Loader A] Initializing Standard Loader for split 'c_base'...\n",
            " -> 300000 total images.\n",
            " -> 6000 classes.\n",
            "Loader A (Standard) check: OK.\n",
            "\n",
            "[Loader B] Initializing Episodic Generator for split 'c_base'...\n",
            " -> Configured 5-Way 1-Shot Tasks (Native PyTorch).\n",
            "Loader B (Episodic) check: OK. Batch shape: torch.Size([10, 3, 84, 84])\n",
            "\n",
            "[Loader C] Creating Fixed Few-Shot Task from 'c_novel'...\n",
            " -> Support Set: 25 images, Query Set: 75 images\n",
            "Loader C (Fixed) check: OK.\n",
            "\n",
            "Step 2.3 Complete (Dependencies Fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 4: INTELLIGENT PRE-TRAINING (MAX PERF & MEMORY SAFE)\n",
        "#################################################################\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "import gc  # <--- WICHTIG für Garbage Collection\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\")\n",
        "\n",
        "# --- 0. DRIVE & PATH SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. SEED SETUP ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# --- 2. HARDWARE DETECTION (TUNED FOR A100) ---\n",
        "def get_optimal_config():\n",
        "    cpu_count = os.cpu_count()\n",
        "    optimal_workers = min(cpu_count, 8)\n",
        "    device_name = \"CPU\"\n",
        "    batch_size = 16\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        device_name = gpu_name\n",
        "        # --- TUNING ---\n",
        "        if \"A100\" in gpu_name:\n",
        "            batch_size = 512  # <--- Aggressiver für A100 (40GB VRAM erlaubt das locker)\n",
        "        elif \"T4\" in gpu_name:\n",
        "            batch_size = 128\n",
        "        else:\n",
        "            batch_size = 64\n",
        "    else:\n",
        "        print(\"WARNING: No GPU detected!\")\n",
        "\n",
        "    return device_name, batch_size, optimal_workers\n",
        "\n",
        "detected_device, auto_bs, auto_workers = get_optimal_config()\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "\n",
        "    # --- CONTROL CENTER ---\n",
        "    'DRY_RUN': False,            # <--- REAL TRAINING\n",
        "    'NUM_EPOCHS': 20,\n",
        "    # ----------------------\n",
        "\n",
        "    'BATCH_SIZE': auto_bs,\n",
        "    'NUM_WORKERS': auto_workers,\n",
        "    'DEVICE_NAME': detected_device,\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'PATIENCE': 5,\n",
        "    'SUBSETS': [0.25, 0.50, 1.0],\n",
        "\n",
        "    'CHECKPOINT_DIR_LOC': '/content/checkpoints',\n",
        "    'CHECKPOINT_DIR_DRIVE': MODELS_DIR\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG['CHECKPOINT_DIR_LOC'], exist_ok=True)\n",
        "\n",
        "print(f\"\\nSystem Configuration:\")\n",
        "print(f\" -> Hardware:    {CONFIG['DEVICE_NAME']}\")\n",
        "print(f\" -> Batch Size:  {CONFIG['BATCH_SIZE']} (Optimized)\")\n",
        "print(f\" -> Workers:     {CONFIG['NUM_WORKERS']}\")\n",
        "print(f\" -> Mode:        {'DRY RUN' if CONFIG['DRY_RUN'] else 'REAL TRAINING'}\")\n",
        "\n",
        "# --- 4. MEMORY CLEANUP HELPER (NEW) ---\n",
        "def cleanup_memory():\n",
        "    \"\"\"Forces Garbage Collection and clears GPU Cache.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # Optional: Print stats to verify\n",
        "    # print(f\"   [Mem] Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# --- 5. MODEL FACTORY ---\n",
        "def get_base_model(arch_name, num_classes, pretrained=True):\n",
        "    # Loading logic same as before\n",
        "    if arch_name == 'resnet34':\n",
        "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet18':\n",
        "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    elif arch_name == 'resnet50':\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
        "        in_features = model.fc.in_features\n",
        "    else:\n",
        "        raise ValueError(\"Arch not supported\")\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# --- 6. DATA LOADER HELPER ---\n",
        "def get_subset_loader(fraction):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "    full_ds = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split='c_base', transform=train_transforms)\n",
        "\n",
        "    total_base_classes = len(full_ds.allowed_ids)\n",
        "    target_num = int(total_base_classes * fraction)\n",
        "    subset_ids = full_ds.allowed_ids[:target_num]\n",
        "\n",
        "    # Filter samples (Memory efficient filtering logic)\n",
        "    # We recreate the list to drop references to unused samples\n",
        "    new_samples = [s for s in full_ds.samples if s['label'] < target_num]\n",
        "    full_ds.samples = new_samples\n",
        "    full_ds.allowed_ids = subset_ids\n",
        "    full_ds.label_map = {orig: new for new, orig in enumerate(subset_ids)}\n",
        "\n",
        "    print(f\"\\n[Data] Subset {fraction*100}%: {len(new_samples)} images, {target_num} classes.\")\n",
        "\n",
        "    num_val = int(0.1 * len(full_ds))\n",
        "    train_ds, val_ds = random_split(full_ds, [len(full_ds)-num_val, num_val],\n",
        "                                    generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True,\n",
        "                              num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False,\n",
        "                            num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, target_num\n",
        "\n",
        "# --- 7. ROBUST CHECKPOINTING ---\n",
        "def safe_copy_to_drive(local_path, filename, max_retries=5):\n",
        "    drive_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], filename)\n",
        "    if not os.path.exists(CONFIG['CHECKPOINT_DIR_DRIVE']):\n",
        "        try: os.makedirs(CONFIG['CHECKPOINT_DIR_DRIVE'], exist_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            shutil.copy(local_path, drive_path)\n",
        "            if os.path.exists(drive_path) and os.path.getsize(drive_path) > 0:\n",
        "                print(f\"   -> Drive Copy: SUCCESS\")\n",
        "                return\n",
        "        except Exception as e:\n",
        "            wait_time = 3 * attempt\n",
        "            print(f\"   [Retry {attempt}] Copy failed ({e}). Waiting {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    print(f\"   [CRITICAL ERROR] Failed to copy {filename} to Drive.\")\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model, filename):\n",
        "    local_path = os.path.join(CONFIG['CHECKPOINT_DIR_LOC'], filename)\n",
        "    torch.save(model.state_dict(), local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "# --- 8. TRAINING ENGINE ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, target_epochs, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    run_tag = \"_dryrun\" if CONFIG['DRY_RUN'] else \"\"\n",
        "    ckpt_filename = f\"{model_name}{run_tag}_checkpoint.pth\"\n",
        "    best_filename = f\"{model_name}{run_tag}_best.pth\"\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_acc = -1.0\n",
        "\n",
        "    # Resume Logic\n",
        "    drive_ckpt_path = os.path.join(CONFIG['CHECKPOINT_DIR_DRIVE'], ckpt_filename)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"\\n[RESUME] Found: {ckpt_filename}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(drive_ckpt_path, map_location=device)\n",
        "            saved_epoch = checkpoint['epoch']\n",
        "\n",
        "            if CONFIG['DRY_RUN']:\n",
        "                print(f\"   -> (Dry Run) Resetting loop despite found epoch {saved_epoch+1}.\")\n",
        "                start_epoch = 0\n",
        "                best_acc = checkpoint.get('best_acc', -1.0)\n",
        "            else:\n",
        "                if saved_epoch >= (target_epochs - 1):\n",
        "                    print(f\"   -> Fully trained ({saved_epoch+1} epochs). Skipping.\")\n",
        "                    return model\n",
        "                start_epoch = saved_epoch + 1\n",
        "                best_acc = checkpoint.get('best_acc', 0.0)\n",
        "\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            if 'scaler_state_dict' in checkpoint:\n",
        "                scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "            print(f\"   -> Resuming with Best Acc: {best_acc:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   [ERROR] Checkpoint corrupted ({e}). Fresh start.\")\n",
        "    else:\n",
        "        print(f\"\\n[START] Fresh start for {model_name}.\")\n",
        "\n",
        "    effective_epochs = 2 if CONFIG['DRY_RUN'] else target_epochs\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(start_epoch, effective_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{effective_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        limit_batches = 5 if CONFIG['DRY_RUN'] else None\n",
        "\n",
        "        pbar = tqdm(train_loader, leave=False, desc=\"Training\")\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if limit_batches and i >= limit_batches: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        iter_size = (limit_batches * CONFIG['BATCH_SIZE']) if limit_batches else len(train_loader.dataset)\n",
        "        if iter_size == 0: iter_size = 1\n",
        "        epoch_acc = running_corrects.double() / iter_size\n",
        "        epoch_loss = running_loss / iter_size\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        val_limit = 5 if CONFIG['DRY_RUN'] else None\n",
        "        val_count = 0\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            if val_limit and i >= val_limit: break\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "            val_count += inputs.size(0)\n",
        "        val_acc = val_corrects.double() / val_count if val_count > 0 else 0.0\n",
        "        print(f\"   Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save Checkpoint\n",
        "        full_state = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'best_acc': best_acc\n",
        "        }\n",
        "        save_checkpoint(full_state, ckpt_filename)\n",
        "\n",
        "        # Save Best Model logic\n",
        "        save_condition = False\n",
        "        if val_acc > best_acc: save_condition = True\n",
        "        elif CONFIG['DRY_RUN'] and val_acc >= best_acc: save_condition = True\n",
        "        elif best_acc == -1.0: save_condition = True\n",
        "\n",
        "        if save_condition:\n",
        "            best_acc = val_acc\n",
        "            save_best_model(model, best_filename)\n",
        "            print(f\"   [New Best] Saved {best_filename}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if not CONFIG['DRY_RUN'] and patience_counter >= CONFIG['PATIENCE']:\n",
        "            print(f\"   [Early Stopping] Reached patience limit.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training Finished. Final Best Acc: {best_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 9. EXECUTION LOOP (WITH CLEANUP) ---\n",
        "for fraction in CONFIG['SUBSETS']:\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    print(f\"\\n{'='*40}\\nRUN: {subset_name}\\n{'='*40}\")\n",
        "\n",
        "    train_dl, val_dl, num_cls = get_subset_loader(fraction)\n",
        "    model = get_base_model(CONFIG['ARCH'], num_classes=num_cls)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "    # Train\n",
        "    train_model(model, train_dl, val_dl, criterion, optimizer, lr_scheduler, CONFIG['NUM_EPOCHS'], subset_name)\n",
        "\n",
        "    # --- MEMORY CLEANUP ---\n",
        "    print(f\"   [Cleanup] Clearing GPU memory after {subset_name}...\")\n",
        "    del model\n",
        "    del optimizer\n",
        "    del criterion\n",
        "    del train_dl\n",
        "    del val_dl\n",
        "    cleanup_memory() # Call helper to force GC and Empty Cache\n",
        "    print(f\"   [Cleanup] Done. Ready for next model.\\n\")\n",
        "\n",
        "print(\"\\nPHASE 4 COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbiMjGzgloUw",
        "outputId": "ad288dc8-2c66-424f-e37e-9fbfe36ff51d"
      },
      "id": "BbiMjGzgloUw",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHASE 4: Pipeline 0 - Base Model Pre-Training ---\n",
            "\n",
            "System Configuration:\n",
            " -> Hardware:    NVIDIA A100-SXM4-40GB\n",
            " -> Batch Size:  512 (Optimized)\n",
            " -> Workers:     8\n",
            " -> Mode:        REAL TRAINING\n",
            "\n",
            "========================================\n",
            "RUN: M_base_25\n",
            "========================================\n",
            "\n",
            "[Data] Subset 25.0%: 75000 images, 1500 classes.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 199MB/s]\n",
            "/tmp/ipython-input-3002377437.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESUME] Found: M_base_25_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_25...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_50\n",
            "========================================\n",
            "\n",
            "[Data] Subset 50.0%: 150000 images, 3000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_50_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_50...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "========================================\n",
            "RUN: M_base_100\n",
            "========================================\n",
            "\n",
            "[Data] Subset 100.0%: 300000 images, 6000 classes.\n",
            "\n",
            "[RESUME] Found: M_base_100_checkpoint.pth\n",
            "   -> Fully trained (20 epochs). Skipping.\n",
            "   [Cleanup] Clearing GPU memory after M_base_100...\n",
            "   [Cleanup] Done. Ready for next model.\n",
            "\n",
            "\n",
            "PHASE 4 COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Pfad anpassen falls nötig\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "MODELS_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning', 'models', 'base_models')\n",
        "\n",
        "print(f\"Lese Ergebnisse aus: {MODELS_DIR}\\n\")\n",
        "\n",
        "subsets = [25, 50, 100]\n",
        "\n",
        "for s in subsets:\n",
        "    # Wir suchen nach der _checkpoint Datei, da diese die Metadaten hat\n",
        "    filename = f\"M_base_{s}_checkpoint.pth\"\n",
        "    path = os.path.join(MODELS_DIR, filename)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            # Wir laden auf CPU, das geht schneller\n",
        "            checkpoint = torch.load(path, map_location='cpu')\n",
        "\n",
        "            acc = checkpoint.get('best_acc', -1)\n",
        "            epoch = checkpoint.get('epoch', -1)\n",
        "\n",
        "            print(f\"Modell {s}%:\")\n",
        "            print(f\"  -> Best Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "            print(f\"  -> Gestoppt nach Epoche: {epoch+1}\")\n",
        "            print(\"-\" * 30)\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Lesen von {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"WARNUNG: Checkpoint {filename} nicht gefunden. Nur _best.pth vorhanden?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtyyskInML8W",
        "outputId": "180d516f-9581-4bb7-d6b9-8b259c0dc73b"
      },
      "id": "XtyyskInML8W",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lese Ergebnisse aus: /content/drive/MyDrive/Deep Learning/models/base_models\n",
            "\n",
            "Modell 25%:\n",
            "  -> Best Accuracy: 0.4552 (45.52%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 50%:\n",
            "  -> Best Accuracy: 0.3140 (31.40%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n",
            "Modell 100%:\n",
            "  -> Best Accuracy: 0.3681 (36.81%)\n",
            "  -> Gestoppt nach Epoche: 20\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 5: MAML (LOAD ONCE, TRAIN MANY)\n",
        "#################################################################\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "print(\"\\n--- PHASE 5: Pipeline 1 - Meta-Learning (Single Load Architecture) ---\")\n",
        "\n",
        "# --- 0. SETUP ---\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "BASE_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'base_models')\n",
        "MAML_MODELS_DIR = os.path.join(PROJECT_DIR, 'models', 'maml_models')\n",
        "CHECKPOINT_DIR_LOC = '/content/checkpoints'\n",
        "\n",
        "os.makedirs(MAML_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR_LOC, exist_ok=True)\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MAML_CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "    'DRY_RUN': False,\n",
        "    'META_ITERATIONS': 1500,\n",
        "    'VAL_INTERVAL': 100,\n",
        "    'META_BATCH_SIZE': 16,\n",
        "    'IMG_SIZE': 84,\n",
        "    'PRELOAD_RAM': True,\n",
        "\n",
        "    # Tuned Params\n",
        "    'META_LR': 0.001,\n",
        "    'INNER_LR': 0.01,\n",
        "    'INNER_STEPS': 5,\n",
        "    'GRAD_CLIP': 1.0,\n",
        "    'WEIGHT_DECAY': 1e-4,\n",
        "\n",
        "    'WAYS': 5, 'SHOTS': 5, 'QUERY_SHOTS': 15,\n",
        "    'SUBSETS': [0.25, 0.50, 1.0]\n",
        "}\n",
        "\n",
        "if MAML_CONFIG['DRY_RUN']:\n",
        "    MAML_CONFIG['META_ITERATIONS'] = 5\n",
        "    MAML_CONFIG['VAL_INTERVAL'] = 1\n",
        "    MAML_CONFIG['META_BATCH_SIZE'] = 2\n",
        "\n",
        "print(f\"Config: {MAML_CONFIG['META_ITERATIONS']} Iters | Batch {MAML_CONFIG['META_BATCH_SIZE']}\")\n",
        "\n",
        "\n",
        "# --- 2. DATASET (NO FILTERING - LOADS EVERYTHING) ---\n",
        "class CachedMetaINatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, partition_file, split='c_base', transform=None, preload=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load Partition\n",
        "        with open(partition_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        self.allowed_ids = data['sets'][split] # Always load FULL split\n",
        "        self.id_to_path = data['id_to_path']\n",
        "\n",
        "        # Index Files\n",
        "        self.samples = []\n",
        "        # Global map for the full split\n",
        "        self.label_map = {orig: new for new, orig in enumerate(self.allowed_ids)}\n",
        "\n",
        "        for original_id in self.allowed_ids:\n",
        "            rel_path = self.id_to_path[str(original_id)]\n",
        "            abs_path = os.path.join(self.root_dir, rel_path)\n",
        "            if os.path.exists(abs_path):\n",
        "                for img in os.listdir(abs_path):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        self.samples.append({\n",
        "                            'path': os.path.join(abs_path, img),\n",
        "                            'label': self.label_map[original_id] # 0..5999\n",
        "                        })\n",
        "\n",
        "        # RAM Preloading\n",
        "        self.cache = {}\n",
        "        self.preload = preload\n",
        "        if self.preload:\n",
        "            print(f\"   [RAM] Preloading ALL {len(self.samples)} images for '{split}'...\")\n",
        "            for i, s in enumerate(tqdm(self.samples, desc=\"Caching\")):\n",
        "                img = Image.open(s['path']).convert('RGB')\n",
        "                img = img.resize((MAML_CONFIG['IMG_SIZE'], MAML_CONFIG['IMG_SIZE']))\n",
        "                self.cache[i] = img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.preload:\n",
        "            image = self.cache[idx]\n",
        "        else:\n",
        "            image = Image.open(self.samples[idx]['path']).convert('RGB')\n",
        "            image = image.resize((MAML_CONFIG['IMG_SIZE'], MAML_CONFIG['IMG_SIZE']))\n",
        "\n",
        "        label = self.samples[idx]['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# --- 3. TASK GENERATOR (THE FILTER IS HERE NOW) ---\n",
        "class A100TaskGenerator:\n",
        "    def __init__(self, dataset, ways, shots, query_shots, fraction=1.0):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # --- SECURITY LOGIC ---\n",
        "        # 1. Determine max allowed class index\n",
        "        # Since dataset labels are 0..5999 (sorted), we can just slice the range.\n",
        "        total_classes_in_ds = len(dataset.allowed_ids)\n",
        "        allowed_count = int(total_classes_in_ds * fraction)\n",
        "\n",
        "        print(f\"   [Generator] Initialized for {fraction*100}% Subset.\")\n",
        "        print(f\"   [Security] Allowed Classes: 0 to {allowed_count-1} (Total available: {total_classes_in_ds})\")\n",
        "\n",
        "        # 2. Build index map ONLY for allowed classes\n",
        "        self.indices_by_label = {}\n",
        "\n",
        "        # We iterate through samples, but only store indices if label < allowed_count\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl < allowed_count:\n",
        "                if lbl not in self.indices_by_label:\n",
        "                    self.indices_by_label[lbl] = []\n",
        "                self.indices_by_label[lbl].append(idx)\n",
        "\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "        print(f\"   [Security] Generator can access {len(self.classes)} classes.\")\n",
        "\n",
        "    def sample_batch(self, batch_size=16):\n",
        "        all_data, all_labels = [], []\n",
        "        for _ in range(batch_size):\n",
        "            # Sample from RESTRICTED class list\n",
        "            selected_classes = random.sample(self.classes, self.ways)\n",
        "            task_imgs, task_lbls = [], []\n",
        "\n",
        "            for local_label, global_label in enumerate(selected_classes):\n",
        "                indices = self.indices_by_label[global_label]\n",
        "                needed = self.shots + self.query_shots\n",
        "                if len(indices) >= needed:\n",
        "                    selected = random.sample(indices, needed)\n",
        "                else:\n",
        "                    selected = random.choices(indices, k=needed)\n",
        "\n",
        "                for idx in selected:\n",
        "                    img, _ = self.dataset[idx] # Access Global RAM\n",
        "                    task_imgs.append(img)\n",
        "                    task_lbls.append(local_label)\n",
        "\n",
        "            all_data.append(torch.stack(task_imgs))\n",
        "            all_labels.append(torch.tensor(task_lbls))\n",
        "        return torch.stack(all_data), torch.stack(all_labels)\n",
        "\n",
        "\n",
        "# --- 4. GLOBAL INIT (LOAD ONCE!) ---\n",
        "print(\"\\n>>> INITIALIZING GLOBAL DATASETS (ONCE) <<<\")\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "global_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(NORM_MEAN, NORM_STD)\n",
        "])\n",
        "\n",
        "# Load FULL c_base (300k images) into RAM\n",
        "GLOBAL_TRAIN_DS = CachedMetaINatDataset(\n",
        "    DATASET_ROOT, PARTITION_FILE_PATH, split='c_base',\n",
        "    transform=global_transform, preload=MAML_CONFIG['PRELOAD_RAM']\n",
        ")\n",
        "\n",
        "# Load FULL c_val into RAM (Separate object)\n",
        "GLOBAL_VAL_DS = CachedMetaINatDataset(\n",
        "    DATASET_ROOT, PARTITION_FILE_PATH, split='c_val',\n",
        "    transform=global_transform, preload=MAML_CONFIG['PRELOAD_RAM']\n",
        ")\n",
        "print(\">>> GLOBAL DATASETS READY <<<\\n\")\n",
        "\n",
        "\n",
        "# --- 5. HELPERS ---\n",
        "def safe_copy_to_drive(local_path, filename):\n",
        "    drive_path = os.path.join(MAML_MODELS_DIR, filename)\n",
        "    try: shutil.copy(local_path, drive_path)\n",
        "    except: pass\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "\n",
        "def save_best_model(model_state, filename):\n",
        "    local_path = os.path.join(CHECKPOINT_DIR_LOC, filename)\n",
        "    torch.save(model_state, local_path)\n",
        "    safe_copy_to_drive(local_path, filename)\n",
        "    print(f\"   [New Best] Saved {filename}\")\n",
        "\n",
        "def load_base_model_for_maml(fraction, arch='resnet34'):\n",
        "    subset_name = f\"M_base_{int(fraction*100)}\"\n",
        "    candidates = [f\"{subset_name}_best.pth\", f\"{subset_name}_checkpoint.pth\", f\"{subset_name}_dryrun_best.pth\"]\n",
        "    path = None\n",
        "    for c in candidates:\n",
        "        p = os.path.join(BASE_MODELS_DIR, c)\n",
        "        if os.path.exists(p):\n",
        "            path = p; break\n",
        "    if path is None: raise FileNotFoundError(f\"No base model for {subset_name}\")\n",
        "\n",
        "    full_classes = 6000\n",
        "    num_classes = int(full_classes * fraction)\n",
        "    model = models.resnet34(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
        "    return model\n",
        "\n",
        "# --- 6. STEPS ---\n",
        "def optimized_maml_step(meta_model, tasks_data, tasks_labels, meta_optimizer, criterion, device, meta_batch_size):\n",
        "    meta_loss_total = 0.0\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "    support_indices = list(range(ways * shots)) # Simplified slicing assumption\n",
        "    # Wait, proper slicing needed.\n",
        "    # Data: [MetaBatch, Ways*(S+Q), C, H, W]\n",
        "    # We must iterate manually or use advanced indexing.\n",
        "\n",
        "    # Correct Slicing Pre-calc\n",
        "    support_indices, query_indices = [], []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    for i in range(meta_batch_size):\n",
        "        supp_X = tasks_data[i][support_indices]\n",
        "        supp_y = tasks_labels[i][support_indices]\n",
        "        query_X = tasks_data[i][query_indices]\n",
        "        query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train()\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "        for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        q_preds = fast_model(query_X)\n",
        "        q_loss = criterion(q_preds, query_y)\n",
        "        meta_loss_total += q_loss.item()\n",
        "        q_loss.backward()\n",
        "\n",
        "        for mp, fp in zip(meta_model.parameters(), fast_model.parameters()):\n",
        "            if fp.grad is not None:\n",
        "                grad = fp.grad.detach() / meta_batch_size\n",
        "                if mp.grad is None: mp.grad = grad\n",
        "                else: mp.grad += grad\n",
        "        del fast_model, inner_opt\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(meta_model.parameters(), MAML_CONFIG['GRAD_CLIP'])\n",
        "    meta_optimizer.step()\n",
        "    return meta_loss_total / meta_batch_size\n",
        "\n",
        "def evaluate_optimized(meta_model, val_generator, criterion, device):\n",
        "    meta_model.eval()\n",
        "    num_val_batches = 1 if MAML_CONFIG['DRY_RUN'] else 5\n",
        "    total_acc = 0.0\n",
        "    meta_batch = MAML_CONFIG['META_BATCH_SIZE']\n",
        "\n",
        "    # Indices logic\n",
        "    ways, shots, queries = MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS']\n",
        "    support_indices, query_indices = [], []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    for _ in range(num_val_batches):\n",
        "        tasks_data, tasks_labels = val_generator.sample_batch(meta_batch)\n",
        "        tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "        batch_acc = 0.0\n",
        "\n",
        "        for i in range(meta_batch):\n",
        "            supp_X = tasks_data[i][support_indices]\n",
        "            supp_y = tasks_labels[i][support_indices]\n",
        "            query_X = tasks_data[i][query_indices]\n",
        "            query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "            fast_model = copy.deepcopy(meta_model)\n",
        "            fast_model.train()\n",
        "            inner_opt = optim.SGD(fast_model.parameters(), lr=MAML_CONFIG['INNER_LR'])\n",
        "\n",
        "            for _ in range(MAML_CONFIG['INNER_STEPS']):\n",
        "                preds = fast_model(supp_X)\n",
        "                loss = criterion(preds, supp_y)\n",
        "                inner_opt.zero_grad()\n",
        "                loss.backward()\n",
        "                inner_opt.step()\n",
        "\n",
        "            fast_model.eval()\n",
        "            with torch.no_grad():\n",
        "                q_preds = fast_model(query_X)\n",
        "                _, predicted = torch.max(q_preds.data, 1)\n",
        "                batch_acc += (predicted == query_y).sum().item() / query_y.size(0)\n",
        "            del fast_model, inner_opt\n",
        "\n",
        "        total_acc += batch_acc\n",
        "    meta_model.train()\n",
        "    return total_acc / (num_val_batches * meta_batch)\n",
        "\n",
        "\n",
        "# --- 7. MAIN LOOP ---\n",
        "def run_maml_training(fraction):\n",
        "    maml_name = f\"M_maml_{int(fraction*100)}\"\n",
        "    run_tag = \"_dryrun\" if MAML_CONFIG['DRY_RUN'] else \"\"\n",
        "    ckpt_file = f\"{maml_name}{run_tag}_checkpoint.pth\"\n",
        "    best_file = f\"{maml_name}{run_tag}_best.pth\"\n",
        "\n",
        "    print(f\"\\n{'='*40}\\nMETA-TRAINING: {maml_name}{run_tag}\\n{'='*40}\")\n",
        "\n",
        "    # --- NEW: CREATE GENERATOR FROM GLOBAL DATASET ---\n",
        "    # Pass the GLOBAL dataset + fraction. The generator handles the filtering logic.\n",
        "    train_gen = A100TaskGenerator(\n",
        "        GLOBAL_TRAIN_DS, # <--- USE GLOBAL OBJECT\n",
        "        MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS'],\n",
        "        fraction=fraction # <--- FILTER APPLIED HERE\n",
        "    )\n",
        "\n",
        "    val_gen = A100TaskGenerator(\n",
        "        GLOBAL_VAL_DS,\n",
        "        MAML_CONFIG['WAYS'], MAML_CONFIG['SHOTS'], MAML_CONFIG['QUERY_SHOTS'],\n",
        "        fraction=1.0 # Validation always uses full val set\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    meta_model = load_base_model_for_maml(fraction, MAML_CONFIG['ARCH'])\n",
        "    meta_model = meta_model.to(device)\n",
        "\n",
        "    meta_optimizer = optim.AdamW(meta_model.parameters(), lr=MAML_CONFIG['META_LR'], weight_decay=MAML_CONFIG['WEIGHT_DECAY'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_iter = 0\n",
        "    best_val_acc = -1.0\n",
        "\n",
        "    drive_ckpt_path = os.path.join(MAML_MODELS_DIR, ckpt_file)\n",
        "    if os.path.exists(drive_ckpt_path):\n",
        "        print(f\"[RESUME] Found {ckpt_file}\")\n",
        "        try:\n",
        "            ckpt = torch.load(drive_ckpt_path, map_location=device)\n",
        "            if MAML_CONFIG['DRY_RUN']:\n",
        "                 start_iter = 0\n",
        "                 best_val_acc = ckpt.get('best_val_acc', -1.0)\n",
        "            else:\n",
        "                 start_iter = ckpt['iteration'] + 1\n",
        "                 best_val_acc = ckpt.get('best_val_acc', 0.0)\n",
        "                 meta_model.load_state_dict(ckpt['model_state_dict'])\n",
        "                 meta_optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "                 print(f\"   -> Resuming from iter {start_iter} (Best Val Acc: {best_val_acc:.4f})\")\n",
        "        except Exception: pass\n",
        "\n",
        "    total_iters = 5 if MAML_CONFIG['DRY_RUN'] else MAML_CONFIG['META_ITERATIONS']\n",
        "    pbar = tqdm(range(start_iter, total_iters), desc=f\"Training\")\n",
        "\n",
        "    for i in pbar:\n",
        "        tasks_data, tasks_labels = train_gen.sample_batch(MAML_CONFIG['META_BATCH_SIZE'])\n",
        "        tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "\n",
        "        loss = optimized_maml_step(meta_model, tasks_data, tasks_labels, meta_optimizer, criterion, device, MAML_CONFIG['META_BATCH_SIZE'])\n",
        "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
        "\n",
        "        if i % MAML_CONFIG['VAL_INTERVAL'] == 0 or i == total_iters - 1:\n",
        "            val_acc = evaluate_optimized(meta_model, val_gen, criterion, device)\n",
        "            tqdm.write(f\"   Iter {i}: Meta Loss {loss:.4f} | Val Acc {val_acc:.4f} (Best: {best_val_acc:.4f})\")\n",
        "\n",
        "            state = {'iteration': i, 'model_state_dict': meta_model.state_dict(), 'optimizer_state_dict': meta_optimizer.state_dict(), 'best_val_acc': best_val_acc}\n",
        "            save_checkpoint(state, ckpt_file)\n",
        "\n",
        "            save_condition = False\n",
        "            if val_acc > best_val_acc: save_condition = True\n",
        "            elif best_val_acc == -1.0: save_condition = True\n",
        "            elif MAML_CONFIG['DRY_RUN'] and val_acc >= best_val_acc: save_condition = True\n",
        "\n",
        "            if save_condition:\n",
        "                best_val_acc = val_acc\n",
        "                save_best_model(meta_model.state_dict(), best_file)\n",
        "\n",
        "    print(f\"Training Complete. Best Val Acc: {best_val_acc:.4f}\")\n",
        "    del meta_model, meta_optimizer, train_gen, val_gen\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 8. EXECUTION ---\n",
        "for fraction in MAML_CONFIG['SUBSETS']:\n",
        "    try:\n",
        "        run_maml_training(fraction)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {fraction}: {e}\")\n",
        "\n",
        "print(\"\\nPHASE 5 COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492,
          "referenced_widgets": [
            "150e87db600f458c849a8497e7d41c75",
            "aeba9f81767e47e1ad76a44474d9a067",
            "a0fd35ea38084e389dd148cdd045d1b4",
            "812bc6ad4ee0490da49142c1cdc8cf60",
            "557822110d9940f0bf7bafe13a15a259",
            "601364d6716c4418b3de93056cd7b034",
            "90b4e993b279491aa20cce4cd01ec3be",
            "f342a0d3d87047b890c33ff7739630ac",
            "35bc8994559d471e882133299cca7480",
            "c2febfc116b14764b7b05458154cc5d6",
            "993476129c9d4b07927e737d0554940f"
          ]
        },
        "id": "PLNuhzHGHswM",
        "outputId": "f997b2f1-6889-4bea-d129-91a446d29db4"
      },
      "id": "PLNuhzHGHswM",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PHASE 5: Pipeline 1 - Meta-Learning (Single Load Architecture) ---\n",
            "Config: 1500 Iters | Batch 16\n",
            "\n",
            ">>> INITIALIZING GLOBAL DATASETS (ONCE) <<<\n",
            "   [RAM] Preloading ALL 300000 images for 'c_base'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Caching:   0%|          | 0/300000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "150e87db600f458c849a8497e7d41c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2843864832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# Load FULL c_base (300k images) into RAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m GLOBAL_TRAIN_DS = CachedMetaINatDataset(\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mDATASET_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARTITION_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_base'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAML_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PRELOAD_RAM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2843864832.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, partition_file, split, transform, preload)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   [RAM] Preloading ALL {len(self.samples)} images for '{split}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Caching\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAML_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IMG_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAML_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IMG_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  PHASE 5.2: HIGH-PERFORMANCE TUNING (A100 OPTIMIZED)\n",
        "#################################################################\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"\\n--- PHASE 5.2: High-Performance Tuning ---\")\n",
        "\n",
        "# --- 1. OPTIMIZED DATA GENERATOR ---\n",
        "# Wir erweitern den Generator, um 'Chunks' von Tasks zu liefern\n",
        "class A100TaskGenerator:\n",
        "    def __init__(self, dataset, ways, shots, query_shots):\n",
        "        self.dataset = dataset\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Caching indices for speed\n",
        "        self.indices_by_label = {}\n",
        "        for idx, sample in enumerate(dataset.samples):\n",
        "            lbl = sample['label']\n",
        "            if lbl not in self.indices_by_label: self.indices_by_label[lbl] = []\n",
        "            self.indices_by_label[lbl].append(idx)\n",
        "        self.classes = list(self.indices_by_label.keys())\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        \"\"\"Generates a whole BATCH of tasks at once to minimize CPU overhead.\"\"\"\n",
        "        all_data = []\n",
        "        all_labels = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            selected_classes = random.sample(self.classes, self.ways)\n",
        "            task_imgs, task_lbls = [], []\n",
        "\n",
        "            for local_label, global_label in enumerate(selected_classes):\n",
        "                indices = self.indices_by_label[global_label]\n",
        "                needed = self.shots + self.query_shots\n",
        "                # Fast sampling\n",
        "                selected = random.sample(indices, needed) if len(indices) >= needed else random.choices(indices, k=needed)\n",
        "\n",
        "                for idx in selected:\n",
        "                    img, _ = self.dataset[idx]\n",
        "                    task_imgs.append(img)\n",
        "                    task_lbls.append(local_label)\n",
        "\n",
        "            # Stack images for this task: [N_Samples, C, H, W]\n",
        "            all_data.append(torch.stack(task_imgs))\n",
        "            all_labels.append(torch.tensor(task_lbls))\n",
        "\n",
        "        # Return stacked meta-batch: [MetaBatch, N_Samples, C, H, W]\n",
        "        return torch.stack(all_data), torch.stack(all_labels)\n",
        "\n",
        "# Wrapper to create the optimized generator\n",
        "def get_fast_taskset(split, ways=5, shots=5, query_shots=15):\n",
        "    # Re-use existing MetaINatDataset logic\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((84, 84)), # MAML standard size (faster)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
        "    ])\n",
        "    ds = MetaINatDataset(DATASET_ROOT, PARTITION_FILE_PATH, split=split, transform=train_transforms)\n",
        "    return A100TaskGenerator(ds, ways, shots, query_shots)\n",
        "\n",
        "\n",
        "# --- 2. OPTIMIZED STEP FUNCTION ---\n",
        "def tuning_step_optimized(meta_model, task_generator, meta_optimizer, criterion, device, current_cfg, meta_batch_size):\n",
        "    meta_loss_total = 0.0\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    # 1. FETCH ALL DATA AT ONCE (CPU -> GPU Transfer happens ONCE)\n",
        "    tasks_data, tasks_labels = task_generator.sample_batch(batch_size=meta_batch_size)\n",
        "    tasks_data, tasks_labels = tasks_data.to(device), tasks_labels.to(device)\n",
        "\n",
        "    # Pre-calculate indices\n",
        "    ways, shots, queries = 5, 5, 15\n",
        "    support_indices = []\n",
        "    query_indices = []\n",
        "    for w in range(ways):\n",
        "        base = w * (shots + queries)\n",
        "        support_indices.extend(range(base, base + shots))\n",
        "        query_indices.extend(range(base + shots, base + shots + queries))\n",
        "\n",
        "    # 2. LOOP ON GPU (No more CPU waiting)\n",
        "    for i in range(meta_batch_size):\n",
        "        # Slicing on GPU is fast\n",
        "        supp_X = tasks_data[i][support_indices]\n",
        "        supp_y = tasks_labels[i][support_indices]\n",
        "        query_X = tasks_data[i][query_indices]\n",
        "        query_y = tasks_labels[i][query_indices]\n",
        "\n",
        "        # --- Standard MAML Inner Loop ---\n",
        "        fast_model = copy.deepcopy(meta_model)\n",
        "        fast_model.train()\n",
        "        inner_opt = optim.SGD(fast_model.parameters(), lr=current_cfg['INNER_LR'])\n",
        "\n",
        "        for _ in range(current_cfg['INNER_STEPS']):\n",
        "            preds = fast_model(supp_X)\n",
        "            loss = criterion(preds, supp_y)\n",
        "            inner_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            inner_opt.step()\n",
        "\n",
        "        q_preds = fast_model(query_X)\n",
        "        q_loss = criterion(q_preds, query_y)\n",
        "        meta_loss_total += q_loss.item()\n",
        "        q_loss.backward()\n",
        "\n",
        "        # Accumulate Gradients\n",
        "        for mp, fp in zip(meta_model.parameters(), fast_model.parameters()):\n",
        "            if fp.grad is not None:\n",
        "                grad = fp.grad.detach() / meta_batch_size\n",
        "                if mp.grad is None: mp.grad = grad\n",
        "                else: mp.grad += grad\n",
        "\n",
        "        del fast_model, inner_opt # Free VRAM immediately\n",
        "\n",
        "    meta_optimizer.step()\n",
        "    return meta_loss_total / meta_batch_size\n",
        "\n",
        "\n",
        "# --- 3. TUNING CONFIGURATION ---\n",
        "GRID_SEARCH = {\n",
        "    'META_LR': [1e-3, 1e-4],\n",
        "    'INNER_LR': [0.1, 0.01],\n",
        "    'INNER_STEPS': [1, 5]\n",
        "}\n",
        "\n",
        "TUNING_CONFIG = {\n",
        "    'ARCH': 'resnet34',\n",
        "    'SUBSET': 0.25,\n",
        "    'ITERS': 100,       # Weniger Iterationen, da Batch Size größer!\n",
        "    'META_BATCH': 32,   # <--- HIER IST DER TURBO! (Standard war 4)\n",
        "    'VAL_INT': 25,\n",
        "    'DRY_RUN': True\n",
        "}\n",
        "\n",
        "if TUNING_CONFIG['DRY_RUN']:\n",
        "    TUNING_CONFIG['ITERS'] = 2\n",
        "    TUNING_CONFIG['VAL_INT'] = 1\n",
        "    TUNING_CONFIG['META_BATCH'] = 2\n",
        "\n",
        "\n",
        "# --- 4. RUN ENGINE ---\n",
        "def run_tuning(combo_params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    meta_model = load_base_model_for_maml(TUNING_CONFIG['SUBSET'], TUNING_CONFIG['ARCH'])\n",
        "    meta_model = meta_model.to(device)\n",
        "    meta_optimizer = optim.Adam(meta_model.parameters(), lr=combo_params['META_LR'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use optimized generator\n",
        "    train_gen = get_fast_taskset(split='c_base')\n",
        "    val_gen = get_fast_taskset(split='c_val') # Reuse for simplicity\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    pbar = tqdm(range(TUNING_CONFIG['ITERS']), desc=\"Run\", leave=False)\n",
        "    for i in pbar:\n",
        "        loss = tuning_step_optimized(meta_model, train_gen, meta_optimizer, criterion, device, combo_params, TUNING_CONFIG['META_BATCH'])\n",
        "        pbar.set_postfix(loss=f\"{loss:.3f}\")\n",
        "\n",
        "        if (i+1) % TUNING_CONFIG['VAL_INT'] == 0:\n",
        "            # Quick Validation (Zero-Shot Proxy for speed)\n",
        "            meta_model.eval()\n",
        "            correct, total = 0, 0\n",
        "            # Validate on 1 batch of tasks\n",
        "            v_data, v_lbl = val_gen.sample_batch(4)\n",
        "            v_data, v_lbl = v_data.to(device), v_lbl.to(device)\n",
        "            with torch.no_grad():\n",
        "                # Flatten batch for simple forward pass check\n",
        "                B, N, C, H, W = v_data.shape\n",
        "                logits = meta_model(v_data.view(-1, C, H, W))\n",
        "                targets = v_lbl.view(-1)\n",
        "                _, preds = torch.max(logits, 1)\n",
        "                correct += (preds == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "            acc = correct / total\n",
        "            if acc > best_acc: best_acc = acc\n",
        "            meta_model.train()\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "print(f\"Starting A100 Optimized Tuning (Batch Size {TUNING_CONFIG['META_BATCH']})...\")\n",
        "results = []\n",
        "keys = list(GRID_SEARCH.keys())\n",
        "combos = list(itertools.product(*GRID_SEARCH.values()))\n",
        "\n",
        "combo_pbar = tqdm(combos, desc=\"Total Progress\")\n",
        "for c in combo_pbar:\n",
        "    params = dict(zip(keys, c))\n",
        "    try:\n",
        "        score = run_tuning(params)\n",
        "        res = params.copy()\n",
        "        res['Acc'] = score\n",
        "        results.append(res)\n",
        "        combo_pbar.write(f\" -> {params} | Acc: {score:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# --- 6. RESULTS ---\n",
        "if results:\n",
        "    df = pd.DataFrame(results).sort_values(by='Acc', ascending=False)\n",
        "    print(\"\\n\", df)\n",
        "\n",
        "    # Heatmap logic\n",
        "    unique_steps = df['INNER_STEPS'].unique()\n",
        "    fig, axes = plt.subplots(1, len(unique_steps), figsize=(12, 5), sharey=True)\n",
        "    if len(unique_steps) == 1: axes = [axes]\n",
        "    for i, steps in enumerate(sorted(unique_steps)):\n",
        "        ax = axes[i]\n",
        "        subset = df[df['INNER_STEPS'] == steps]\n",
        "        pivot = subset.pivot(index='META_LR', columns='INNER_LR', values='Acc')\n",
        "        sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"viridis\", ax=ax)\n",
        "        ax.set_title(f\"Inner Steps: {steps}\")\n",
        "    plt.show()\n",
        "\n",
        "    win = df.iloc[0]\n",
        "    print(f\"\\nWINNER: MetaLR={win['META_LR']}, InnerLR={win['INNER_LR']}, Steps={int(win['INNER_STEPS'])}\")"
      ],
      "metadata": {
        "id": "W3hgf4PdMhLb"
      },
      "id": "W3hgf4PdMhLb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "150e87db600f458c849a8497e7d41c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aeba9f81767e47e1ad76a44474d9a067",
              "IPY_MODEL_a0fd35ea38084e389dd148cdd045d1b4",
              "IPY_MODEL_812bc6ad4ee0490da49142c1cdc8cf60"
            ],
            "layout": "IPY_MODEL_557822110d9940f0bf7bafe13a15a259"
          }
        },
        "aeba9f81767e47e1ad76a44474d9a067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601364d6716c4418b3de93056cd7b034",
            "placeholder": "​",
            "style": "IPY_MODEL_90b4e993b279491aa20cce4cd01ec3be",
            "value": "Caching:   3%"
          }
        },
        "a0fd35ea38084e389dd148cdd045d1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f342a0d3d87047b890c33ff7739630ac",
            "max": 300000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35bc8994559d471e882133299cca7480",
            "value": 8667
          }
        },
        "812bc6ad4ee0490da49142c1cdc8cf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2febfc116b14764b7b05458154cc5d6",
            "placeholder": "​",
            "style": "IPY_MODEL_993476129c9d4b07927e737d0554940f",
            "value": " 8667/300000 [00:30&lt;16:22, 296.50it/s]"
          }
        },
        "557822110d9940f0bf7bafe13a15a259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "601364d6716c4418b3de93056cd7b034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b4e993b279491aa20cce4cd01ec3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f342a0d3d87047b890c33ff7739630ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bc8994559d471e882133299cca7480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2febfc116b14764b7b05458154cc5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "993476129c9d4b07927e737d0554940f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}