{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcvonrohr/DeepLearning/blob/main/meta_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54242527",
      "metadata": {
        "id": "54242527",
        "outputId": "3903597a-4155-4773-b15b-81962cfcbcc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "#################################################################\n",
        "#  STEP 2.1: PREPARE LOCAL VM\n",
        "#################################################################\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "print(\"Connecting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"...Google Drive connected.\")\n",
        "\n",
        "# --- 2. Define Key Paths ---\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/'\n",
        "PROJECT_DIR = os.path.join(GDRIVE_ROOT, 'Deep Learning')\n",
        "DATASETS_ROOT_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
        "INAT_ROOT_DIR = os.path.join(DATASETS_ROOT_DIR, 'inaturalist')\n",
        "\n",
        "# Source: The COMPRESSED archives\n",
        "ARCHIVES_DIR_ON_DRIVE = os.path.join(INAT_ROOT_DIR, 'archives')\n",
        "\n",
        "# Target: The LOCAL VM fast disk\n",
        "LOCAL_DATA_ROOT = '/content/data'\n",
        "# This is the final path your PyTorch code will use:\n",
        "FINAL_DATA_PATH = os.path.join(LOCAL_DATA_ROOT, 'inaturalist_unpacked')\n",
        "\n",
        "# Define source/destination paths\n",
        "TAR_FILES = {\n",
        "    \"2021_train_mini\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_train_mini.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_train_mini.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_train_mini')\n",
        "    },\n",
        "    \"2021_valid\": {\n",
        "        \"src\": os.path.join(ARCHIVES_DIR_ON_DRIVE, '2021_valid.tar.gz'),\n",
        "        \"dest_tar\": os.path.join(LOCAL_DATA_ROOT, '2021_valid.tar.gz'),\n",
        "        \"check_unpacked\": os.path.join(FINAL_DATA_PATH, '2021_valid')\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 3. Create Local Directories on VM ---\n",
        "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(FINAL_DATA_PATH, exist_ok=True)\n",
        "print(f\"Local data directory created at: {FINAL_DATA_PATH}\")\n",
        "\n",
        "# --- 4. Copy, Unpack, and Clean up for each file ---\n",
        "for name, paths in TAR_FILES.items():\n",
        "    print(f\"\\n--- Processing {name} ---\")\n",
        "\n",
        "    if os.path.exists(paths[\"check_unpacked\"]):\n",
        "        print(f\"'{name}' is already unpacked in local VM. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 4a. Copy .tar.gz from Drive to local VM\n",
        "    print(f\"Copying '{name}.tar.gz' from Drive to local VM...\")\n",
        "    start_time = time.time()\n",
        "    !cp \"{paths['src']}\" \"{paths['dest_tar']}\"\n",
        "    print(f\"...Copy complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4b. Unpack the file on the local VM\n",
        "    print(f\"Unpacking '{name}.tar.gz' locally...\")\n",
        "    start_time = time.time()\n",
        "    !tar -xzf \"{paths['dest_tar']}\" -C \"{FINAL_DATA_PATH}\"\n",
        "    print(f\"...Unpacking complete. Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    # 4c. Delete the local .tar.gz file to save VM space\n",
        "    print(f\"Deleting local tarball '{paths['dest_tar']}'...\")\n",
        "    !rm \"{paths['dest_tar']}\"\n",
        "    print(\"...Local tarball deleted.\")\n",
        "\n",
        "# --- 5. Verify and Set Path for Training ---\n",
        "print(\"\\n--- Final Data Setup Verification ---\")\n",
        "print(f\"Dataset is ready for training at: {FINAL_DATA_PATH}\")\n",
        "!ls -lh \"{FINAL_DATA_PATH}\"\n",
        "print(\"\\nLocal VM Disk Space Usage:\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#  STEP 2.2: SCIENTIFIC DATA PARTITIONING\n",
        "#################################################################\n",
        "print(\"\\n--- STEP 2.2: Loading/Creating Scientific Class Partition ---\")\n",
        "\n",
        "# --- 6. Define Paths for Partition File ---\n",
        "# We create a 'project_meta' folder on GDrive to store helper files\n",
        "META_DIR_ON_DRIVE = os.path.join(PROJECT_DIR, 'project_meta')\n",
        "os.makedirs(META_DIR_ON_DRIVE, exist_ok=True)\n",
        "\n",
        "PARTITION_FILE_PATH = os.path.join(META_DIR_ON_DRIVE, 'inat_class_split.json')\n",
        "print(f\"Looking for partition file at: {PARTITION_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "1_ARRY2GUvXt"
      },
      "id": "1_ARRY2GUvXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Load or Create the Partition ---\n",
        "class_split = {}\n",
        "RANDOM_SEED = 42 # Guarantees the shuffle is always the same\n",
        "\n",
        "if os.path.exists(PARTITION_FILE_PATH):\n",
        "    # Load the existing file from GDrive\n",
        "    print(\"Found existing partition file. Loading...\")\n",
        "    with open(PARTITION_FILE_PATH, 'r') as f:\n",
        "        class_split = json.load(f)\n",
        "        # Ensure keys are loaded as lists (just in case)\n",
        "        class_split = {k: list(v) for k, v in class_split.items()}\n",
        "\n",
        "else:\n",
        "    # Create the partition for the first time\n",
        "    print(\"No partition file found. Creating new partition...\")\n",
        "\n",
        "    # 7a. Load the dataset's metadata file from the local VM\n",
        "    # This file contains the list of all 10,000 categories\n",
        "    metadata_file = os.path.join(FINAL_DATA_PATH, '2021_train_mini', '2021_train_mini.json')\n",
        "\n",
        "    if not os.path.exists(metadata_file):\n",
        "        print(f\"ERROR: Metadata file not found at {metadata_file}\")\n",
        "        # Stop execution if something went wrong in Step 2.1\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
        "\n",
        "    with open(metadata_file, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # 7b. Get all category IDs (0-9999)\n",
        "    # The 'categories' list in the JSON is ordered by ID from 0 to 9999\n",
        "    num_classes = len(metadata['categories'])\n",
        "    if num_classes != 10000:\n",
        "        print(f\"WARNING: Expected 10,000 classes, but found {num_classes}.\")\n",
        "\n",
        "    all_class_ids = list(range(num_classes))\n",
        "\n",
        "    # 7c. Shuffle the class list reproducibly\n",
        "    print(f\"Shuffling {num_classes} class IDs with random seed {RANDOM_SEED}...\")\n",
        "    random.seed(RANDOM_SEED)\n",
        "    random.shuffle(all_class_ids)\n",
        "\n",
        "    # 7d. Split into C_base, C_val, C_novel\n",
        "    c_base_ids = all_class_ids[:6000]\n",
        "    c_val_ids = all_class_ids[6000:8000]\n",
        "    c_novel_ids = all_class_ids[8000:]\n",
        "\n",
        "    class_split = {\n",
        "        'c_base': sorted(c_base_ids), # Sort for easier inspection\n",
        "        'c_val': sorted(c_val_ids),\n",
        "        'c_novel': sorted(c_novel_ids)\n",
        "    }\n",
        "\n",
        "    # 7e. Save the new partition file to Google Drive\n",
        "    print(f\"Saving new partition file to: {PARTITION_FILE_PATH}\")\n",
        "    with open(PARTITION_FILE_PATH, 'w') as f:\n",
        "        json.dump(class_split, f, indent=4)"
      ],
      "metadata": {
        "id": "987lbsNFU4Qf"
      },
      "id": "987lbsNFU4Qf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Verification ---\n",
        "print(\"\\n--- Partitioning Complete ---\")\n",
        "print(f\"Total C_base classes:  {len(class_split['c_base'])}\")\n",
        "print(f\"Total C_val classes:   {len(class_split['c_val'])}\")\n",
        "print(f\"Total C_novel classes: {len(class_split['c_novel'])}\")\n",
        "\n",
        "# Check for overlaps (should be 0)\n",
        "base_val_overlap = set(class_split['c_base']) & set(class_split['c_val'])\n",
        "base_novel_overlap = set(class_split['c_base']) & set(class_split['c_novel'])\n",
        "val_novel_overlap = set(class_split['c_val']) & set(class_split['c_novel'])\n",
        "\n",
        "print(f\"Overlap (Base-Val):    {len(base_val_overlap)}\")\n",
        "print(f\"Overlap (Base-Novel):  {len(base_novel_overlap)}\")\n",
        "print(f\"Overlap (Val-Novel):   {len(val_novel_overlap)}\")\n",
        "\n",
        "print(\"\\nReady to proceed with Data Loader.\")"
      ],
      "metadata": {
        "id": "w8xIL6YPU6OM"
      },
      "id": "w8xIL6YPU6OM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}